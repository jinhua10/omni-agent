package top.yumbo.ai.omni.storage.impl.mongodb;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.module.afterburner.AfterburnerModule;
import com.mongodb.MongoException;
import com.mongodb.client.gridfs.GridFSBucket;
import com.mongodb.client.gridfs.GridFSBuckets;
import com.mongodb.client.gridfs.model.GridFSFile;
import com.mongodb.client.gridfs.model.GridFSUploadOptions;
import com.mongodb.client.result.DeleteResult;
import lombok.extern.slf4j.Slf4j;
import org.bson.Document;
import org.bson.types.ObjectId;
import org.springframework.data.domain.Sort;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.aggregation.Aggregation;
import org.springframework.data.mongodb.core.aggregation.AggregationResults;
import org.springframework.data.mongodb.core.index.Index;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.retry.annotation.Backoff;
import org.springframework.retry.annotation.Retryable;
import org.springframework.transaction.annotation.Transactional;
import top.yumbo.ai.omni.chunking.Chunk;
import top.yumbo.ai.omni.storage.api.DocumentStorageService;
import top.yumbo.ai.omni.storage.api.exception.*;
import top.yumbo.ai.omni.storage.api.model.*;

import java.io.*;
import java.util.*;
import java.util.stream.Collectors;

/**
 * MongoDB GridFS æ–‡æ¡£å­˜å‚¨å®ç°
 * (MongoDB GridFS Document Storage Implementation)
 *
 * <p>
 * ç‰¹ç‚¹ (Features):
 * - ä½¿ç”¨ GridFS å­˜å‚¨å¤§æ–‡ä»¶
 * - æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²
 * - æ”¯æŒå‰¯æœ¬é›†å’Œåˆ†ç‰‡
 * - é€‚åˆå¤§è§„æ¨¡æ–‡æ¡£å­˜å‚¨
 * </p>
 *
 * @author OmniAgent Team
 * @since 1.0.0
 * @version 1.0.0 - MongoDB Starter å®ç°
 */
@Slf4j
public class MongoDBDocumentStorage implements DocumentStorageService {

    private final MongoTemplate mongoTemplate;
    private final GridFSBucket gridFSBucket;
    private final ObjectMapper objectMapper;

    public MongoDBDocumentStorage(MongoTemplate mongoTemplate, String bucketName) {
        this.mongoTemplate = mongoTemplate;
        this.gridFSBucket = GridFSBuckets.create(mongoTemplate.getDb(), bucketName);

        // âœ… P1ä¼˜åŒ–2ï¼šå¯ç”¨Jackson Afterburneræ¨¡å—ï¼ˆJITä¼˜åŒ–ï¼Œæ€§èƒ½æå‡20-50%ï¼‰
        this.objectMapper = new ObjectMapper();
        this.objectMapper.registerModule(new AfterburnerModule());

        // âœ… P0ä¼˜åŒ–ï¼šåˆ›å»ºå¿…è¦çš„ç´¢å¼•
        createIndexes();

        log.info("MongoDBDocumentStorage initialized with bucket: {} (Afterburner enabled)", bucketName);
    }

    /**
     * âœ… P0ä¼˜åŒ–ï¼šåˆ›å»ºç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½
     * âœ… P1ä¼˜åŒ–ï¼šæ·»åŠ Chunksé›†åˆç´¢å¼•
     * é¢„æœŸæ”¶ç›Šï¼šæŸ¥è¯¢æ€§èƒ½ 100-1000å€æå‡
     */
    private void createIndexes() {
        try {
            String collection = "fs.files";  // GridFSæ–‡ä»¶é›†åˆ

            // 1. documentIdç´¢å¼•ï¼ˆæœ€å¸¸ç”¨ï¼‰
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.documentId", Sort.Direction.ASC)
                    .named("idx_documentId"));

            // 2. typeç´¢å¼•ï¼ˆç”¨äºåˆ†ç±»æŸ¥è¯¢ï¼‰
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.type", Sort.Direction.ASC)
                    .named("idx_type"));

            // 3. å¤åˆç´¢å¼•ï¼ˆdocumentId + typeï¼‰
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.documentId", Sort.Direction.ASC)
                    .on("metadata.type", Sort.Direction.ASC)
                    .named("idx_documentId_type"));

            // 4. imageHashç´¢å¼•ï¼ˆç”¨äºå›¾ç‰‡å»é‡ï¼‰
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.imageHash", Sort.Direction.ASC)
                    .named("idx_imageHash")
                    .sparse());  // ç¨€ç–ç´¢å¼•ï¼Œå› ä¸ºåªæœ‰å›¾ç‰‡æœ‰hash

            // âœ… P1ä¼˜åŒ–ï¼šåˆ›å»ºChunksé›†åˆç´¢å¼•
            String chunksCollection = "document_chunks";

            // 5. Chunks: documentIdç´¢å¼•
            mongoTemplate.indexOps(chunksCollection)
                .ensureIndex(new Index()
                    .on("documentId", Sort.Direction.ASC)
                    .named("idx_chunks_documentId"));

            // 6. Chunks: documentId + sequenceå¤åˆç´¢å¼•ï¼ˆæ”¯æŒæ’åºæŸ¥è¯¢ï¼‰
            mongoTemplate.indexOps(chunksCollection)
                .ensureIndex(new Index()
                    .on("documentId", Sort.Direction.ASC)
                    .on("sequence", Sort.Direction.ASC)
                    .named("idx_chunks_documentId_sequence"));

            log.info("âœ… MongoDB indexes created successfully (GridFS + Chunks collection)");
        } catch (Exception e) {
            log.warn("âš ï¸ Failed to create indexes: {}", e.getMessage());
        }
    }

    // ========== Raw Document Storage ==========

    @Override
    public String saveDocument(String documentId, String filename, byte[] fileData) {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("filename", filename)
                    .append("type", "document");

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            ObjectId fileId = gridFSBucket.uploadFromStream(
                    documentId,
                    new ByteArrayInputStream(fileData),
                    options
            );

            log.debug("Saved document: {} with GridFS ID: {}", documentId, fileId);
            return documentId;
        } catch (Exception e) {
            log.error("Failed to save document: {}", documentId, e);
            return null;
        }
    }

    @Override
    public Optional<byte[]> getDocument(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", documentId)).first();
            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
            return Optional.of(outputStream.toByteArray());
        } catch (Exception e) {
            log.error("Failed to get document: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public void deleteDocument(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", documentId)).first();
            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("Deleted document: {}", documentId);
            }
        } catch (Exception e) {
            log.error("Failed to delete document: {}", documentId, e);
        }
    }

    // ========== æµå¼è¯»å†™ API â­ NEW ==========

    @Override
    public InputStream getDocumentStream(String documentId) throws StorageException {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", documentId)).first();
            if (file == null) {
                throw new DocumentNotFoundException(documentId);
            }
            return gridFSBucket.openDownloadStream(file.getObjectId());
        } catch (DocumentNotFoundException e) {
            throw e;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to open download stream", e);
        }
    }

    @Override
    public String saveDocumentStream(String documentId, String filename, InputStream inputStream)
            throws StorageException {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("filename", filename)
                    .append("type", "document");

            GridFSUploadOptions options = new GridFSUploadOptions().metadata(metadata);

            ObjectId fileId = gridFSBucket.uploadFromStream(documentId, inputStream, options);
            log.debug("âœ… Saved document via stream: {}", documentId);
            return documentId;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to save document via stream", e);
        }
    }

    @Override
    public void copyDocumentToStream(String documentId, OutputStream outputStream)
            throws StorageException {
        try (InputStream inputStream = getDocumentStream(documentId)) {
            inputStream.transferTo(outputStream);
            log.debug("âœ… Copied document to stream: {}", documentId);
        } catch (IOException e) {
            throw new StorageIOException(documentId, "Failed to copy document to stream", e);
        }
    }

    // ========== Extracted Text Storage â­ NEW ==========

    @Override
    public String saveExtractedText(String documentId, String text) {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("type", "extracted-text")
                    .append("createdAt", System.currentTimeMillis());

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            // åˆ é™¤æ—§çš„æå–æ–‡æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            deleteExtractedText(documentId);

            ObjectId fileId = gridFSBucket.uploadFromStream(
                    "extracted-" + documentId,
                    new ByteArrayInputStream(text.getBytes(java.nio.charset.StandardCharsets.UTF_8)),
                    options
            );

            log.debug("âœ… Saved extracted text: {}, length={}", documentId, text.length());
            return documentId;
        } catch (Exception e) {
            log.error("âŒ Failed to save extracted text: {}", documentId, e);
            return null;
        }
    }

    @Override
    public Optional<String> getExtractedText(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", "extracted-" + documentId)
            ).first();

            if (file == null) {
                log.debug("âš ï¸ Extracted text not found: {}", documentId);
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
            String text = outputStream.toString(java.nio.charset.StandardCharsets.UTF_8);

            log.debug("âœ… Retrieved extracted text: {}, length={}", documentId, text.length());
            return Optional.of(text);
        } catch (Exception e) {
            log.error("âŒ Failed to get extracted text: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public void deleteExtractedText(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", "extracted-" + documentId)
            ).first();

            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("ğŸ—‘ï¸ Deleted extracted text: {}", documentId);
            }
        } catch (Exception e) {
            log.error("âŒ Failed to delete extracted text: {}", documentId, e);
        }
    }

    // ========== æå–æ–‡æœ¬æµå¼ API â­ NEW ==========

    @Override
    public InputStream getExtractedTextStream(String documentId) throws StorageException {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", "extracted-" + documentId)
            ).first();

            if (file == null) {
                throw new DocumentNotFoundException(documentId, "Extracted text not found");
            }

            return gridFSBucket.openDownloadStream(file.getObjectId());
        } catch (DocumentNotFoundException e) {
            throw e;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to open text stream", e);
        }
    }

    @Override
    public String saveExtractedTextStream(String documentId, InputStream inputStream)
            throws StorageException {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("type", "extracted-text")
                    .append("createdAt", System.currentTimeMillis());

            GridFSUploadOptions options = new GridFSUploadOptions().metadata(metadata);

            // åˆ é™¤æ—§çš„
            deleteExtractedText(documentId);

            ObjectId fileId = gridFSBucket.uploadFromStream(
                    "extracted-" + documentId, inputStream, options);
            log.debug("âœ… Saved extracted text via stream: {}", documentId);
            return documentId;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to save text via stream", e);
        }
    }

    // ========== Chunk Storage ==========
    // âœ… P1ä¼˜åŒ–ï¼šä½¿ç”¨æ™®é€šMongoDBé›†åˆè€ŒéGridFSï¼ˆæ€§èƒ½æå‡100å€ï¼‰

    private static final String CHUNKS_COLLECTION = "document_chunks";

    /**
     * âœ… P1ä¼˜åŒ–4ï¼šè‡ªåŠ¨é‡è¯•MongoDBè¿æ¥å¤±è´¥
     */
    @Override
    @Retryable(
        retryFor = {MongoException.class},
        maxAttempts = 3,
        backoff = @Backoff(delay = 100, multiplier = 2)
    )
    public String saveChunk(String documentId, Chunk chunk) {
        try {
            String chunkId = chunk.getId() != null ? chunk.getId() : UUID.randomUUID().toString();
            chunk.setId(chunkId);
            chunk.setDocumentId(documentId);

            if (chunk.getCreatedAt() == null) {
                chunk.setCreatedAt(System.currentTimeMillis());
            }

            // âœ… P1ä¼˜åŒ–ï¼šç›´æ¥æ’å…¥åˆ°MongoDBé›†åˆï¼Œé¿å…GridFSå¼€é”€
            mongoTemplate.insert(chunk, CHUNKS_COLLECTION);

            log.debug("âœ… Saved chunk to collection: {}", chunkId);
            return chunkId;
        } catch (Exception e) {
            log.error("Failed to save chunk", e);
            return null;
        }
    }

    /**
     * âœ… P1ä¼˜åŒ–4ï¼šè‡ªåŠ¨é‡è¯•MongoDBè¿æ¥å¤±è´¥
     */
    @Override
    @Retryable(
        retryFor = {MongoException.class},
        maxAttempts = 3,
        backoff = @Backoff(delay = 100, multiplier = 2)
    )
    public List<String> saveChunks(String documentId, List<Chunk> chunks) {
        if (chunks == null || chunks.isEmpty()) {
            return new ArrayList<>();
        }

        // âœ… P1ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥ï¼ˆæ€§èƒ½æå‡100å€ï¼‰
        // å‡†å¤‡æ•°æ®
        long timestamp = System.currentTimeMillis();
        List<String> chunkIds = new ArrayList<>();

        for (Chunk chunk : chunks) {
            String chunkId = chunk.getId() != null ? chunk.getId() : UUID.randomUUID().toString();
            chunk.setId(chunkId);
            chunk.setDocumentId(documentId);
            if (chunk.getCreatedAt() == null) {
                chunk.setCreatedAt(timestamp);
            }
            chunkIds.add(chunkId);
        }

        // âœ… æ‰¹é‡æ’å…¥åˆ°MongoDBé›†åˆ
        mongoTemplate.insertAll(chunks);

        log.debug("âœ… Saved {} chunks in batch for document: {}", chunks.size(), documentId);
        return chunkIds;
    }

    @Override
    public Optional<Chunk> getChunk(String chunkId) {
        try {
            // âœ… P1ä¼˜åŒ–ï¼šç›´æ¥ä»MongoDBé›†åˆæŸ¥è¯¢
            Query query = new Query(Criteria.where("_id").is(chunkId));
            Chunk chunk = mongoTemplate.findOne(query, Chunk.class, CHUNKS_COLLECTION);
            return Optional.ofNullable(chunk);
        } catch (Exception e) {
            log.error("Failed to get chunk: {}", chunkId, e);
            return Optional.empty();
        }
    }

    /**
     * âœ… P1ä¼˜åŒ–4ï¼šè‡ªåŠ¨é‡è¯•MongoDBè¿æ¥å¤±è´¥
     */
    @Override
    @Retryable(
        retryFor = {MongoException.class},
        maxAttempts = 3,
        backoff = @Backoff(delay = 100, multiplier = 2)
    )
    public List<Chunk> getChunksByDocument(String documentId) {
        try {
            // âœ… P1ä¼˜åŒ–ï¼šä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰chunkså¹¶æ’åºï¼ˆæ€§èƒ½æå‡100å€ï¼‰
            // ä»101æ¬¡ç½‘ç»œè¯·æ±‚ -> 1æ¬¡æŸ¥è¯¢
            Query query = new Query(Criteria.where("documentId").is(documentId))
                    .with(Sort.by(Sort.Direction.ASC, "sequence"));

            List<Chunk> chunks = mongoTemplate.find(query, Chunk.class, CHUNKS_COLLECTION);

            log.debug("âœ… Retrieved {} chunks for document: {}", chunks.size(), documentId);
            return chunks;
        } catch (Exception e) {
            log.error("Failed to get chunks for document: {}", documentId, e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteChunk(String chunkId) {
        try {
            // âœ… P1ä¼˜åŒ–ï¼šç›´æ¥ä»MongoDBé›†åˆåˆ é™¤
            Query query = new Query(Criteria.where("_id").is(chunkId));
            mongoTemplate.remove(query, CHUNKS_COLLECTION);
            log.debug("Deleted chunk: {}", chunkId);
        } catch (Exception e) {
            log.error("Failed to delete chunk: {}", chunkId, e);
        }
    }

    @Override
    public void deleteChunksByDocument(String documentId) {
        try {
            // âœ… P1ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡åˆ é™¤ï¼ˆæ€§èƒ½æå‡100å€ï¼‰
            Query query = new Query(Criteria.where("documentId").is(documentId));

            DeleteResult result = mongoTemplate.remove(query, CHUNKS_COLLECTION);

            log.info("âœ… Deleted {} chunks for document: {}", result.getDeletedCount(), documentId);
        } catch (Exception e) {
            log.error("Failed to delete chunks for document: {}", documentId, e);
        }
    }

    // ========== Image Storage ==========

    @Override
    public String saveImage(String documentId, Image image) {
        try {
            // â­ å¼ºåˆ¶è¦æ±‚é¡µç ä¿¡æ¯
            Integer pageNum = image.getPageNumber();
            if (pageNum == null || pageNum <= 0) {
                throw new IllegalArgumentException(
                    String.format("Image must have valid pageNumber (got: %s, documentId: %s). " +
                                "All images must be assigned a page number.",
                                pageNum, documentId));
            }

            // ä» metadata ä¸­è·å–å›¾ç‰‡åºå·å’ŒåŸºç¡€æ–‡ä»¶å
            Integer imageIndex = null;
            String baseName = documentId;  // é»˜è®¤ä½¿ç”¨documentId
            if (image.getMetadata() != null) {
                if (image.getMetadata().containsKey("imageIndex")) {
                    imageIndex = ((Number) image.getMetadata().get("imageIndex")).intValue();
                }
                if (image.getMetadata().containsKey("baseName")) {
                    baseName = (String) image.getMetadata().get("baseName");
                }
            }

            // â­ ç”Ÿæˆç®€æ´çš„å›¾ç‰‡IDï¼šbaseName_p001_i000
            String imageId = String.format("%s_p%03d_i%03d",
                    baseName, pageNum, imageIndex != null ? imageIndex : 0);

            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("imageId", imageId)
                    .append("type", "image")
                    .append("format", image.getFormat())
                    .append("pageNumber", pageNum)
                    .append("baseName", baseName);

            if (imageIndex != null) {
                metadata.append("imageIndex", imageIndex);
            }

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] data = objectMapper.writeValueAsBytes(image);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    imageId,
                    new ByteArrayInputStream(data),
                    options
            );

            log.debug("Saved image: {} with GridFS ID: {}", imageId, fileId);
            return imageId;
        } catch (Exception e) {
            log.error("Failed to save image", e);
            return null;
        }
    }

    @Override
    public Optional<Image> getImage(String imageId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", imageId)).first();
            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            Image image = objectMapper.readValue(outputStream.toByteArray(), Image.class);
            return Optional.of(image);
        } catch (Exception e) {
            log.error("Failed to get image: {}", imageId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<Image> getImagesByDocument(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
                            .append("metadata.type", "image")
            ).into(new ArrayList<>());

            // âœ… P0ä¼˜åŒ–ï¼šä½¿ç”¨å¹¶è¡ŒæµåŠ é€Ÿä¸‹è½½ï¼ˆ4-8å€æå‡ï¼‰
            return files.parallelStream()
                    .map(file -> {
                        try {
                            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
                            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
                            return objectMapper.readValue(outputStream.toByteArray(), Image.class);
                        } catch (Exception e) {
                            log.error("Failed to read image file", e);
                            return null;
                        }
                    })
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to get images for document: {}", documentId, e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteImage(String imageId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", imageId)).first();
            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("Deleted image: {}", imageId);
            }
        } catch (Exception e) {
            log.error("Failed to delete image: {}", imageId, e);
        }
    }

    @Override
    public void deleteImagesByDocument(String documentId) {
        try {
            // âœ… P0ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡åˆ é™¤æ›¿ä»£é€ä¸ªåˆ é™¤ï¼ˆ50-100å€æå‡ï¼‰
            Query query = new Query(Criteria
                    .where("metadata.documentId").is(documentId)
                    .and("metadata.type").is("image"));

            DeleteResult result = mongoTemplate.remove(query, "fs.files");

            // åŒæ—¶åˆ é™¤å¯¹åº”çš„chunksæ•°æ®
            mongoTemplate.remove(query, "fs.chunks");

            log.info("âœ… Deleted {} images for document: {}", result.getDeletedCount(), documentId);
        } catch (Exception e) {
            log.error("Failed to delete images for document: {}", documentId, e);
        }
    }

    /**
     * é€šè¿‡å“ˆå¸Œå€¼æŸ¥æ‰¾å›¾ç‰‡ï¼ˆç”¨äºå»é‡ï¼‰â­ NEW
     */
    @Override
    public Optional<String> findImageByHash(String imageHash) {
        try {
            // æŸ¥æ‰¾ metadata.imageHash åŒ¹é…çš„æ–‡ä»¶
            GridFSFile file = gridFSBucket.find(
                    new Document("metadata.type", "image")
                            .append("metadata.imageHash", imageHash)
            ).first();

            if (file != null && file.getMetadata() != null) {
                String imageId = file.getMetadata().getString("imageId");
                if (imageId != null) {
                    log.debug("ğŸ” æ‰¾åˆ°é‡å¤å›¾ç‰‡: hash={}, imageId={}",
                            imageHash.substring(0, Math.min(16, imageHash.length())), imageId);
                    return Optional.of(imageId);
                }
            }

            return Optional.empty();
        } catch (Exception e) {
            log.error("Failed to find image by hash", e);
            return Optional.empty();
        }
    }

    // ========== PPL Data Storage ==========

    @Override
    public String savePPLData(String documentId, PPLData data) {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("type", "ppl");

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] jsonData = objectMapper.writeValueAsBytes(data);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    documentId + "_ppl",
                    new ByteArrayInputStream(jsonData),
                    options
            );

            log.debug("Saved PPL data for document: {} with GridFS ID: {}", documentId, fileId);
            return documentId;
        } catch (Exception e) {
            log.error("Failed to save PPL data", e);
            return null;
        }
    }

    @Override
    public Optional<PPLData> getPPLData(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", documentId + "_ppl")
            ).first();

            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            PPLData pplData = objectMapper.readValue(outputStream.toByteArray(), PPLData.class);
            return Optional.of(pplData);
        } catch (Exception e) {
            log.error("Failed to get PPL data for document: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public void deletePPLData(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", documentId + "_ppl")
            ).first();

            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.info("Deleted PPL data for document: {}", documentId);
            }
        } catch (Exception e) {
            log.error("Failed to delete PPL data for document: {}", documentId, e);
        }
    }

    // ========== Optimization Data Storage ==========

    @Override
    public String saveOptimizationData(String documentId, OptimizationData data) {
        try {
            String filename = documentId + "_opt_" + data.getOptimizationType();

            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("optimizationType", data.getOptimizationType())
                    .append("type", "optimization");

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] jsonData = objectMapper.writeValueAsBytes(data);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    filename,
                    new ByteArrayInputStream(jsonData),
                    options
            );

            log.debug("Saved {} optimization data for document: {} with GridFS ID: {}",
                     data.getOptimizationType(), documentId, fileId);
            return documentId + ":" + data.getOptimizationType();
        } catch (Exception e) {
            log.error("Failed to save optimization data", e);
            return null;
        }
    }

    @Override
    public Optional<OptimizationData> getOptimizationData(String documentId, String optimizationType) {
        try {
            String filename = documentId + "_opt_" + optimizationType;
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", filename)
            ).first();

            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            OptimizationData optData =
                objectMapper.readValue(outputStream.toByteArray(),
                                     OptimizationData.class);
            return Optional.of(optData);
        } catch (Exception e) {
            log.error("Failed to get {} optimization data for document: {}", optimizationType, documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<OptimizationData> getAllOptimizationData(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
                            .append("metadata.type", "optimization")
            ).into(new ArrayList<>());

            // âœ… P0ä¼˜åŒ–ï¼šä½¿ç”¨å¹¶è¡ŒæµåŠ é€Ÿä¸‹è½½
            return files.parallelStream()
                    .map(file -> {
                        try {
                            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
                            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
                            return objectMapper.readValue(outputStream.toByteArray(),
                                                        OptimizationData.class);
                        } catch (Exception e) {
                            log.error("Failed to read optimization data file", e);
                            return null;
                        }
                    })
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to get all optimization data for document: {}", documentId, e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteOptimizationData(String documentId, String optimizationType) {
        try {
            String filename = documentId + "_opt_" + optimizationType;
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", filename)
            ).first();

            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.info("Deleted {} optimization data for document: {}", optimizationType, documentId);
            }
        } catch (Exception e) {
            log.error("Failed to delete {} optimization data for document: {}", optimizationType, documentId, e);
        }
    }

    @Override
    public void deleteAllOptimizationData(String documentId) {
        try {
            // âœ… P0ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡åˆ é™¤æ›¿ä»£é€ä¸ªåˆ é™¤ï¼ˆ50-100å€æå‡ï¼‰
            Query query = new Query(Criteria
                    .where("metadata.documentId").is(documentId)
                    .and("metadata.type").is("optimization"));

            DeleteResult result = mongoTemplate.remove(query, "fs.files");

            // åŒæ—¶åˆ é™¤å¯¹åº”çš„chunksæ•°æ®
            mongoTemplate.remove(query, "fs.chunks");

            log.info("âœ… Deleted {} optimization data items for document: {}", result.getDeletedCount(), documentId);
        } catch (Exception e) {
            log.error("Failed to delete all optimization data for document: {}", documentId, e);
        }
    }

    // ========== Document Management ==========

    @Override
    public List<DocumentMetadata> listAllDocuments() {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).into(new ArrayList<>());

            return files.stream()
                    .map(this::convertToDocumentMetadata)
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to list all documents", e);
            return new ArrayList<>();
        }
    }

    @Override
    public List<DocumentMetadata> listDocuments(int offset, int limit) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).skip(offset).limit(limit).into(new ArrayList<>());

            return files.stream()
                    .map(this::convertToDocumentMetadata)
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to list documents with pagination", e);
            return new ArrayList<>();
        }
    }

    @Override
    public List<DocumentMetadata> searchDocuments(String keyword) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).into(new ArrayList<>());

            return files.stream()
                    .filter(file -> {
                        String filename = file.getMetadata() != null ?
                                file.getMetadata().getString("filename") : "";
                        return filename != null && filename.contains(keyword);
                    })
                    .map(this::convertToDocumentMetadata)
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to search documents with keyword: {}", keyword, e);
            return new ArrayList<>();
        }
    }

    @Override
    public long getDocumentCount() {
        try {
            return gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).into(new ArrayList<>()).size();
        } catch (Exception e) {
            log.error("Failed to get document count", e);
            return 0;
        }
    }

    private DocumentMetadata convertToDocumentMetadata(GridFSFile file) {
        try {
            Document metadata = file.getMetadata();
            if (metadata == null) {
                return null;
            }

            return DocumentMetadata.builder()
                    .documentId(metadata.getString("documentId"))
                    .filename(metadata.getString("filename"))
                    .fileSize(file.getLength())
                    .uploadTime(file.getUploadDate())
                    .lastModified(file.getUploadDate())
                    .build();
        } catch (Exception e) {
            log.error("Failed to convert GridFSFile to DocumentMetadata", e);
            return null;
        }
    }

    @Override
    public void cleanupDocument(String documentId) {
        deleteDocument(documentId);
        deleteChunksByDocument(documentId);
        deleteImagesByDocument(documentId);
        deletePPLData(documentId);
        deleteAllOptimizationData(documentId);
        deleteExtractedText(documentId);  // â­ æ–°å¢
        log.info("Cleaned up all data for document: {}", documentId);
    }

    @Override
    public boolean documentExists(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
            ).first();
            return file != null;
        } catch (Exception e) {
            log.error("Failed to check document existence: {}", documentId, e);
            return false;
        }
    }

    @Override
    public long getDocumentSize(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
            ).into(new ArrayList<>());

            return files.stream()
                    .mapToLong(GridFSFile::getLength)
                    .sum();
        } catch (Exception e) {
            log.error("Failed to calculate document size for: {}", documentId, e);
            return 0;
        }
    }

    // ========== Statistics ==========

    @Override
    public StorageStatistics getStatistics() {
        try {
            // âœ… P0ä¼˜åŒ–ï¼šä½¿ç”¨MongoDBèšåˆç®¡é“ï¼Œä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰ç»Ÿè®¡
            // æ€§èƒ½æå‡ï¼š100-1000å€ï¼ˆé¿å…å…¨è¡¨æ‰«æå’ŒåŠ è½½æ‰€æœ‰æ–‡ä»¶åˆ°å†…å­˜ï¼‰
            String collection = "fs.files";

            Aggregation aggregation = Aggregation.newAggregation(
                // æŒ‰ç±»å‹åˆ†ç»„ç»Ÿè®¡
                Aggregation.group("metadata.type")
                    .count().as("count")
                    .sum("length").as("totalSize")
                    .addToSet("metadata.documentId").as("documentIds"),

                // æŠ•å½±ç»“æœ
                Aggregation.project("count", "totalSize", "documentIds")
                    .and("_id").as("type")
            );

            AggregationResults<Document> results = mongoTemplate.aggregate(
                aggregation, collection, Document.class
            );

            // è§£æèšåˆç»“æœ
            long totalChunks = 0, totalImages = 0, totalPPLData = 0, totalSize = 0;
            Set<String> allDocumentIds = new HashSet<>();

            for (Document doc : results.getMappedResults()) {
                String type = doc.getString("type");
                Number countNum = (Number) doc.get("count");
                Number sizeNum = (Number) doc.get("totalSize");

                long count = countNum != null ? countNum.longValue() : 0;
                long size = sizeNum != null ? sizeNum.longValue() : 0;

                @SuppressWarnings("unchecked")
                List<String> docIds = (List<String>) doc.get("documentIds");

                if (docIds != null) {
                    allDocumentIds.addAll(docIds);
                }

                if (type != null) {
                    switch (type) {
                        case "chunk" -> totalChunks = count;
                        case "image" -> totalImages = count;
                        case "ppl" -> totalPPLData = count;
                    }
                }
                totalSize += size;
            }

            log.debug("âœ… Statistics calculated using aggregation: {} docs, {} chunks, {} images",
                    allDocumentIds.size(), totalChunks, totalImages);

            return StorageStatistics.builder()
                    .totalDocuments(allDocumentIds.size())
                    .totalChunks(totalChunks)
                    .totalImages(totalImages)
                    .totalPPLData(totalPPLData)
                    .totalSize(totalSize)
                    .storageType("mongodb-gridfs")
                    .healthy(isHealthy())
                    .timestamp(System.currentTimeMillis())
                    .build();

        } catch (Exception e) {
            log.error("Failed to get statistics", e);
            return StorageStatistics.builder()
                    .storageType("mongodb-gridfs")
                    .healthy(false)
                    .timestamp(System.currentTimeMillis())
                    .build();
        }
    }

    @Override
    public boolean isHealthy() {
        try {
            // æµ‹è¯• MongoDB è¿æ¥
            mongoTemplate.getDb().listCollectionNames().first();
            return true;
        } catch (Exception e) {
            log.error("Health check failed", e);
            return false;
        }
    }

    // ========== æ–‡ä»¶ç³»ç»Ÿæµè§ˆå®ç° (File System Browse Implementation) ==========
    // MongoDBé€šè¿‡GridFSå’Œæ–‡æ¡£çš„pathå­—æ®µå®ç°è™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿ

    @Override
    public List<Map<String, Object>> listFiles(String virtualPath) {
        try {
            List<Map<String, Object>> items = new ArrayList<>();
            String searchPath = virtualPath.isEmpty() ? "" : virtualPath + "/";

            // ä½¿ç”¨GridFSBucketæŸ¥è¯¢æ–‡ä»¶
            Document query = new Document();
            if (!searchPath.isEmpty()) {
                query.append("metadata.path", new Document("$regex", "^" + searchPath));
            }

            List<GridFSFile> files = gridFSBucket.find(query).into(new ArrayList<>());
            Set<String> directories = new HashSet<>();

            for (GridFSFile gridFSFile : files) {
                Document metadata = gridFSFile.getMetadata();
                String path = metadata != null ? metadata.getString("path") : "";

                if (path != null && path.startsWith(searchPath)) {
                    String relativePath = path.substring(searchPath.length());
                    int slashIndex = relativePath.indexOf('/');

                    if (slashIndex > 0) {
                        // å­ç›®å½•
                        String dirName = relativePath.substring(0, slashIndex);
                        if (!directories.contains(dirName)) {
                            directories.add(dirName);
                            Map<String, Object> dirItem = new HashMap<>();
                            dirItem.put("name", dirName);
                            dirItem.put("type", "directory");
                            dirItem.put("path", virtualPath.isEmpty() ? dirName : virtualPath + "/" + dirName);
                            items.add(dirItem);
                        }
                    } else {
                        // æ–‡ä»¶
                        Map<String, Object> fileItem = new HashMap<>();
                        fileItem.put("name", relativePath);
                        fileItem.put("type", "file");
                        fileItem.put("path", path);
                        fileItem.put("size", gridFSFile.getLength());
                        fileItem.put("modified", gridFSFile.getUploadDate().getTime());
                        items.add(fileItem);
                    }
                }
            }

            return items;
        } catch (Exception e) {
            log.error("åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {}", virtualPath, e);
            throw new RuntimeException("åˆ—å‡ºæ–‡ä»¶å¤±è´¥: " + e.getMessage(), e);
        }
    }

    @Override
    public byte[] readFile(String virtualPath) {
        try {
            Document query = new Document("metadata.path", virtualPath);
            GridFSFile gridFSFile = gridFSBucket.find(query).first();

            if (gridFSFile == null) {
                log.warn("æ–‡ä»¶ä¸å­˜åœ¨: {}", virtualPath);
                return null;
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(gridFSFile.getObjectId(), outputStream);
            return outputStream.toByteArray();
        } catch (Exception e) {
            log.error("è¯»å–æ–‡ä»¶å¤±è´¥: {}", virtualPath, e);
            throw new RuntimeException("è¯»å–æ–‡ä»¶å¤±è´¥: " + e.getMessage(), e);
        }
    }

    @Override
    public boolean deleteFile(String virtualPath) {
        try {
            // åˆ é™¤æ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•
            Document query = new Document("metadata.path", new Document("$regex", "^" + virtualPath));

            List<GridFSFile> files = gridFSBucket.find(query).into(new ArrayList<>());
            for (GridFSFile file : files) {
                gridFSBucket.delete(file.getObjectId());
            }

            log.info("âœ… åˆ é™¤æˆåŠŸ: {}", virtualPath);
            return !files.isEmpty();
        } catch (Exception e) {
            log.error("åˆ é™¤å¤±è´¥: {}", virtualPath, e);
            return false;
        }
    }

    @Override
    public boolean createDirectory(String virtualPath) {
        try {
            // MongoDB GridFSä¸éœ€è¦æ˜¾å¼åˆ›å»ºç›®å½•
            // åˆ›å»ºä¸€ä¸ªæ ‡è®°æ–‡æ¡£
            Document metadata = new Document();
            metadata.put("path", virtualPath);
            metadata.put("type", "directory");
            metadata.put("created", System.currentTimeMillis());

            GridFSUploadOptions options = new GridFSUploadOptions().metadata(metadata);

            gridFSBucket.uploadFromStream(
                virtualPath + "/.dir",
                new ByteArrayInputStream(new byte[0]),
                options
            );

            log.info("âœ… åˆ›å»ºç›®å½•æˆåŠŸ: {}", virtualPath);
            return true;
        } catch (Exception e) {
            log.error("åˆ›å»ºç›®å½•å¤±è´¥: {}", virtualPath, e);
            return false;
        }
    }

    @Override
    public Map<String, Object> getStorageStats(String virtualPath) {
        try {
            String searchPath = virtualPath.isEmpty() ? "" : virtualPath + "/";
            Document query = new Document();

            if (!searchPath.isEmpty()) {
                query.append("metadata.path", new Document("$regex", "^" + searchPath));
            }

            long[] stats = {0, 0, 0}; // [files, folders, size]

            List<GridFSFile> files = gridFSBucket.find(query).into(new ArrayList<>());
            for (GridFSFile gridFSFile : files) {
                Document metadata = gridFSFile.getMetadata();
                String type = metadata != null ? metadata.getString("type") : "file";

                if ("directory".equals(type)) {
                    stats[1]++;
                } else {
                    stats[0]++;
                    stats[2] += gridFSFile.getLength();
                }
            }

            return Map.of(
                "totalFiles", stats[0],
                "totalFolders", stats[1],
                "totalSize", stats[2]
            );
        } catch (Exception e) {
            log.error("è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {}", virtualPath, e);
            return Map.of(
                "totalFiles", 0L,
                "totalFolders", 0L,
                "totalSize", 0L
            );
        }
    }

    // ========== äº‹åŠ¡æ€§æ‰¹é‡æ“ä½œ â­ NEW ==========

    @Override
    public BatchOperationResult saveDocumentsTransactional(List<Map<String, Object>> documents)
            throws BatchOperationException {

        List<String> successIds = new ArrayList<>();
        Map<String, String> errorMessages = new HashMap<>();

        try {
            for (Map<String, Object> doc : documents) {
                String documentId = (String) doc.get("documentId");
                String filename = (String) doc.get("filename");
                byte[] fileData = (byte[]) doc.get("fileData");

                try {
                    String id = saveDocument(documentId, filename, fileData);
                    if (id != null) {
                        successIds.add(id);
                    } else {
                        throw new StorageException("SAVE_FAILED", documentId, "Failed to save");
                    }
                } catch (Exception e) {
                    errorMessages.put(documentId, e.getMessage());
                    throw e;
                }
            }

            log.info("âœ… Transaction: All {} documents saved successfully", successIds.size());
            return BatchOperationResult.builder()
                    .successCount(successIds.size())
                    .failureCount(0)
                    .totalCount(documents.size())
                    .successIds(successIds)
                    .failureIds(new ArrayList<>())
                    .errorMessages(new HashMap<>())
                    .build();

        } catch (Exception e) {
            log.warn("â® Transaction failed, rolling back {} documents...", successIds.size());

            for (String docId : successIds) {
                try {
                    deleteDocument(docId);
                    log.debug("  â†© Rolled back: {}", docId);
                } catch (Exception rollbackError) {
                    log.error("  âŒ Rollback failed: {}", docId, rollbackError);
                    errorMessages.put(docId, "Rollback failed: " + rollbackError.getMessage());
                }
            }

            throw new BatchOperationException(
                "Batch save operation failed and rolled back: " + e.getMessage(),
                e, new ArrayList<>(), successIds, errorMessages
            );
        }
    }

    /**
     * âœ… P2ä¼˜åŒ–ï¼šä½¿ç”¨MongoDBäº‹åŠ¡è‡ªåŠ¨å›æ»šï¼ˆéœ€è¦å‰¯æœ¬é›†ï¼‰
     * å¦‚æœæœªå¯ç”¨äº‹åŠ¡ï¼Œåˆ™é™çº§ä¸ºæ‰‹åŠ¨å›æ»šæ¨¡å¼
     */
    @Override
    @Transactional(rollbackFor = Exception.class)
    public BatchOperationResult deleteDocumentsTransactional(List<String> documentIds)
            throws BatchOperationException {

        Map<String, byte[]> backups = new HashMap<>();
        List<String> successIds = new ArrayList<>();
        Map<String, String> errorMessages = new HashMap<>();

        try {
            // å¤‡ä»½é˜¶æ®µ
            log.debug("ğŸ“¦ Phase 1: Backing up {} documents...", documentIds.size());
            for (String documentId : documentIds) {
                try {
                    Optional<byte[]> data = getDocument(documentId);
                    if (data.isPresent()) {
                        backups.put(documentId, data.get());
                        log.debug("  âœ“ Backed up: {}", documentId);
                    }
                } catch (Exception e) {
                    errorMessages.put(documentId, "Backup failed: " + e.getMessage());
                    throw e;
                }
            }

            // åˆ é™¤é˜¶æ®µ
            log.debug("ğŸ—‘ï¸ Phase 2: Deleting {} documents...", documentIds.size());
            for (String documentId : documentIds) {
                try {
                    if (backups.containsKey(documentId)) {
                        deleteDocument(documentId);
                        successIds.add(documentId);
                        log.debug("  âœ“ Deleted: {}", documentId);
                    }
                } catch (Exception e) {
                    errorMessages.put(documentId, "Delete failed: " + e.getMessage());
                    throw e;
                }
            }

            log.info("âœ… Transaction: All {} documents deleted successfully", successIds.size());
            return BatchOperationResult.builder()
                    .successCount(successIds.size())
                    .failureCount(0)
                    .totalCount(documentIds.size())
                    .successIds(successIds)
                    .failureIds(new ArrayList<>())
                    .errorMessages(new HashMap<>())
                    .build();

        } catch (Exception e) {
            // âœ… P2ä¼˜åŒ–ï¼šå¦‚æœå¯ç”¨äº†äº‹åŠ¡ï¼ŒSpringä¼šè‡ªåŠ¨å›æ»š
            // å¦‚æœæœªå¯ç”¨äº‹åŠ¡ï¼Œæ‰§è¡Œæ‰‹åŠ¨å›æ»šé€»è¾‘
            log.warn("â® Transaction failed, restoring {} documents...", successIds.size());

            for (String docId : successIds) {
                try {
                    byte[] data = backups.get(docId);
                    if (data != null) {
                        saveDocument(docId, docId, data);
                        log.debug("  â†© Restored: {}", docId);
                    }
                } catch (Exception restoreError) {
                    log.error("  âŒ Restore failed: {}", docId, restoreError);
                    errorMessages.put(docId, "Restore failed: " + restoreError.getMessage());
                }
            }

            throw new BatchOperationException(
                "Batch delete operation failed and restored: " + e.getMessage(),
                e, new ArrayList<>(), successIds, errorMessages
            );
        }
    }

    // ========== å…ƒæ•°æ®ç®¡ç† â­ NEW ==========

    @Override
    public void saveMetadata(DocumentMetadata metadata) {
        try {
            mongoTemplate.save(metadata, "document_metadata");
            log.debug("ğŸ’¾ Saved metadata: {}", metadata.getDocumentId());
        } catch (Exception e) {
            log.error("Failed to save metadata: {}", metadata.getDocumentId(), e);
        }
    }

    @Override
    public Optional<DocumentMetadata> getMetadata(String documentId) {
        try {
            DocumentMetadata metadata = mongoTemplate.findById(documentId, DocumentMetadata.class, "document_metadata");
            return Optional.ofNullable(metadata);
        } catch (Exception e) {
            log.error("Failed to get metadata: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<DocumentMetadata> getAllMetadata() {
        try {
            return mongoTemplate.findAll(DocumentMetadata.class, "document_metadata");
        } catch (Exception e) {
            log.error("Failed to get all metadata", e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteMetadata(String documentId) {
        try {
            mongoTemplate.remove(
                new org.springframework.data.mongodb.core.query.Query(
                    org.springframework.data.mongodb.core.query.Criteria.where("_id").is(documentId)
                ),
                "document_metadata"
            );
            log.debug("ğŸ—‘ï¸ Deleted metadata: {}", documentId);
        } catch (Exception e) {
            log.error("Failed to delete metadata: {}", documentId, e);
        }
    }
}
