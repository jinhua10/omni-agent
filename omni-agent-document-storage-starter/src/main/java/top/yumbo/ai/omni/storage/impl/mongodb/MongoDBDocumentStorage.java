package top.yumbo.ai.omni.storage.impl.mongodb;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.mongodb.client.gridfs.GridFSBucket;
import com.mongodb.client.gridfs.GridFSBuckets;
import com.mongodb.client.gridfs.model.GridFSFile;
import com.mongodb.client.gridfs.model.GridFSUploadOptions;
import com.mongodb.client.result.DeleteResult;
import lombok.extern.slf4j.Slf4j;
import org.bson.Document;
import org.bson.types.ObjectId;
import org.springframework.data.domain.Sort;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.aggregation.Aggregation;
import org.springframework.data.mongodb.core.aggregation.AggregationResults;
import org.springframework.data.mongodb.core.index.Index;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import top.yumbo.ai.omni.chunking.Chunk;
import top.yumbo.ai.omni.storage.api.model.DocumentMetadata;
import top.yumbo.ai.omni.storage.api.model.OptimizationData;
import top.yumbo.ai.omni.storage.api.DocumentStorageService;
import top.yumbo.ai.omni.storage.api.exception.*;
import top.yumbo.ai.omni.storage.api.model.*;

import java.io.*;
import java.util.*;
import java.util.stream.Collectors;

/**
 * MongoDB GridFS ÊñáÊ°£Â≠òÂÇ®ÂÆûÁé∞
 * (MongoDB GridFS Document Storage Implementation)
 *
 * <p>
 * ÁâπÁÇπ (Features):
 * - ‰ΩøÁî® GridFS Â≠òÂÇ®Â§ßÊñá‰ª∂
 * - ÊîØÊåÅÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤
 * - ÊîØÊåÅÂâØÊú¨ÈõÜÂíåÂàÜÁâá
 * - ÈÄÇÂêàÂ§ßËßÑÊ®°ÊñáÊ°£Â≠òÂÇ®
 * </p>
 *
 * @author OmniAgent Team
 * @since 1.0.0
 * @version 1.0.0 - MongoDB Starter ÂÆûÁé∞
 */
@Slf4j
public class MongoDBDocumentStorage implements DocumentStorageService {

    private final MongoTemplate mongoTemplate;
    private final GridFSBucket gridFSBucket;
    private final ObjectMapper objectMapper;

    public MongoDBDocumentStorage(MongoTemplate mongoTemplate, String bucketName) {
        this.mongoTemplate = mongoTemplate;
        this.gridFSBucket = GridFSBuckets.create(mongoTemplate.getDb(), bucketName);
        this.objectMapper = new ObjectMapper();

        // ‚úÖ P0‰ºòÂåñÔºöÂàõÂª∫ÂøÖË¶ÅÁöÑÁ¥¢Âºï
        createIndexes();

        log.info("MongoDBDocumentStorage initialized with bucket: {}", bucketName);
    }

    /**
     * ‚úÖ P0‰ºòÂåñÔºöÂàõÂª∫Á¥¢Âºï‰ª•ÊèêÂçáÊü•ËØ¢ÊÄßËÉΩ
     * È¢ÑÊúüÊî∂ÁõäÔºöÊü•ËØ¢ÊÄßËÉΩ 100-1000ÂÄçÊèêÂçá
     */
    private void createIndexes() {
        try {
            String collection = "fs.files";  // GridFSÊñá‰ª∂ÈõÜÂêà

            // 1. documentIdÁ¥¢ÂºïÔºàÊúÄÂ∏∏Áî®Ôºâ
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.documentId", Sort.Direction.ASC)
                    .named("idx_documentId"));

            // 2. typeÁ¥¢ÂºïÔºàÁî®‰∫éÂàÜÁ±ªÊü•ËØ¢Ôºâ
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.type", Sort.Direction.ASC)
                    .named("idx_type"));

            // 3. Â§çÂêàÁ¥¢ÂºïÔºàdocumentId + typeÔºâ
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.documentId", Sort.Direction.ASC)
                    .on("metadata.type", Sort.Direction.ASC)
                    .named("idx_documentId_type"));

            // 4. imageHashÁ¥¢ÂºïÔºàÁî®‰∫éÂõæÁâáÂéªÈáçÔºâ
            mongoTemplate.indexOps(collection)
                .ensureIndex(new Index()
                    .on("metadata.imageHash", Sort.Direction.ASC)
                    .named("idx_imageHash")
                    .sparse());  // Á®ÄÁñèÁ¥¢ÂºïÔºåÂõ†‰∏∫Âè™ÊúâÂõæÁâáÊúâhash

            log.info("‚úÖ MongoDB indexes created successfully");
        } catch (Exception e) {
            log.warn("‚ö†Ô∏è Failed to create indexes: {}", e.getMessage());
        }
    }

    // ========== Raw Document Storage ==========

    @Override
    public String saveDocument(String documentId, String filename, byte[] fileData) {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("filename", filename)
                    .append("type", "document");

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            ObjectId fileId = gridFSBucket.uploadFromStream(
                    documentId,
                    new ByteArrayInputStream(fileData),
                    options
            );

            log.debug("Saved document: {} with GridFS ID: {}", documentId, fileId);
            return documentId;
        } catch (Exception e) {
            log.error("Failed to save document: {}", documentId, e);
            return null;
        }
    }

    @Override
    public Optional<byte[]> getDocument(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", documentId)).first();
            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
            return Optional.of(outputStream.toByteArray());
        } catch (Exception e) {
            log.error("Failed to get document: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public void deleteDocument(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", documentId)).first();
            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("Deleted document: {}", documentId);
            }
        } catch (Exception e) {
            log.error("Failed to delete document: {}", documentId, e);
        }
    }

    // ========== ÊµÅÂºèËØªÂÜô API ‚≠ê NEW ==========

    @Override
    public InputStream getDocumentStream(String documentId) throws StorageException {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", documentId)).first();
            if (file == null) {
                throw new DocumentNotFoundException(documentId);
            }
            return gridFSBucket.openDownloadStream(file.getObjectId());
        } catch (DocumentNotFoundException e) {
            throw e;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to open download stream", e);
        }
    }

    @Override
    public String saveDocumentStream(String documentId, String filename, InputStream inputStream)
            throws StorageException {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("filename", filename)
                    .append("type", "document");

            GridFSUploadOptions options = new GridFSUploadOptions().metadata(metadata);

            ObjectId fileId = gridFSBucket.uploadFromStream(documentId, inputStream, options);
            log.debug("‚úÖ Saved document via stream: {}", documentId);
            return documentId;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to save document via stream", e);
        }
    }

    @Override
    public void copyDocumentToStream(String documentId, OutputStream outputStream)
            throws StorageException {
        try (InputStream inputStream = getDocumentStream(documentId)) {
            inputStream.transferTo(outputStream);
            log.debug("‚úÖ Copied document to stream: {}", documentId);
        } catch (IOException e) {
            throw new StorageIOException(documentId, "Failed to copy document to stream", e);
        }
    }

    // ========== Extracted Text Storage ‚≠ê NEW ==========

    @Override
    public String saveExtractedText(String documentId, String text) {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("type", "extracted-text")
                    .append("createdAt", System.currentTimeMillis());

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            // Âà†Èô§ÊóßÁöÑÊèêÂèñÊñáÊú¨ÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
            deleteExtractedText(documentId);

            ObjectId fileId = gridFSBucket.uploadFromStream(
                    "extracted-" + documentId,
                    new ByteArrayInputStream(text.getBytes(java.nio.charset.StandardCharsets.UTF_8)),
                    options
            );

            log.debug("‚úÖ Saved extracted text: {}, length={}", documentId, text.length());
            return documentId;
        } catch (Exception e) {
            log.error("‚ùå Failed to save extracted text: {}", documentId, e);
            return null;
        }
    }

    @Override
    public Optional<String> getExtractedText(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", "extracted-" + documentId)
            ).first();

            if (file == null) {
                log.debug("‚ö†Ô∏è Extracted text not found: {}", documentId);
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
            String text = outputStream.toString(java.nio.charset.StandardCharsets.UTF_8);

            log.debug("‚úÖ Retrieved extracted text: {}, length={}", documentId, text.length());
            return Optional.of(text);
        } catch (Exception e) {
            log.error("‚ùå Failed to get extracted text: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public void deleteExtractedText(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", "extracted-" + documentId)
            ).first();

            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("üóëÔ∏è Deleted extracted text: {}", documentId);
            }
        } catch (Exception e) {
            log.error("‚ùå Failed to delete extracted text: {}", documentId, e);
        }
    }

    // ========== ÊèêÂèñÊñáÊú¨ÊµÅÂºè API ‚≠ê NEW ==========

    @Override
    public InputStream getExtractedTextStream(String documentId) throws StorageException {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", "extracted-" + documentId)
            ).first();

            if (file == null) {
                throw new DocumentNotFoundException(documentId, "Extracted text not found");
            }

            return gridFSBucket.openDownloadStream(file.getObjectId());
        } catch (DocumentNotFoundException e) {
            throw e;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to open text stream", e);
        }
    }

    @Override
    public String saveExtractedTextStream(String documentId, InputStream inputStream)
            throws StorageException {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("type", "extracted-text")
                    .append("createdAt", System.currentTimeMillis());

            GridFSUploadOptions options = new GridFSUploadOptions().metadata(metadata);

            // Âà†Èô§ÊóßÁöÑ
            deleteExtractedText(documentId);

            ObjectId fileId = gridFSBucket.uploadFromStream(
                    "extracted-" + documentId, inputStream, options);
            log.debug("‚úÖ Saved extracted text via stream: {}", documentId);
            return documentId;
        } catch (Exception e) {
            throw new StorageIOException(documentId, "Failed to save text via stream", e);
        }
    }

    // ========== Chunk Storage ==========

    @Override
    public String saveChunk(String documentId, Chunk chunk) {
        try {
            String chunkId = chunk.getId() != null ? chunk.getId() : UUID.randomUUID().toString();

            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("chunkId", chunkId)
                    .append("type", "chunk")
                    .append("sequence", chunk.getSequence());

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] data = objectMapper.writeValueAsBytes(chunk);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    chunkId,
                    new ByteArrayInputStream(data),
                    options
            );

            log.debug("Saved chunk: {} with GridFS ID: {}", chunkId, fileId);
            return chunkId;
        } catch (Exception e) {
            log.error("Failed to save chunk", e);
            return null;
        }
    }

    @Override
    public List<String> saveChunks(String documentId, List<Chunk> chunks) {
        if (chunks == null || chunks.isEmpty()) {
            return new ArrayList<>();
        }

        // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®Âπ∂Ë°åÊµÅÊâπÈáèÂ§ÑÁêÜ
        // Ê≥®ÊÑèÔºöGridFS‰∏çÊîØÊåÅBulkOperationsÔºå‰ΩÜÂèØ‰ª•Âπ∂Ë°å‰∏ä‰º†‰ª•ÊèêÂçáÊÄßËÉΩ
        List<String> chunkIds = chunks.parallelStream()
                .map(chunk -> {
                    String chunkId = saveChunk(documentId, chunk);
                    if (chunkId == null) {
                        log.warn("‚ö†Ô∏è Failed to save chunk for document: {}", documentId);
                    }
                    return chunkId;
                })
                .filter(Objects::nonNull)
                .collect(Collectors.toList());

        log.debug("‚úÖ Saved {} chunks in parallel for document: {}", chunkIds.size(), documentId);
        return chunkIds;
    }

    @Override
    public Optional<Chunk> getChunk(String chunkId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", chunkId)).first();
            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            Chunk chunk = objectMapper.readValue(outputStream.toByteArray(), Chunk.class);
            return Optional.of(chunk);
        } catch (Exception e) {
            log.error("Failed to get chunk: {}", chunkId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<Chunk> getChunksByDocument(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
                            .append("metadata.type", "chunk")
            ).into(new ArrayList<>());

            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®Âπ∂Ë°åÊµÅÂä†ÈÄü‰∏ãËΩΩÔºà4-8ÂÄçÊèêÂçáÔºâ
            return files.parallelStream()
                    .map(file -> {
                        try {
                            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
                            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
                            return objectMapper.readValue(outputStream.toByteArray(), Chunk.class);
                        } catch (Exception e) {
                            log.error("Failed to read chunk file", e);
                            return null;
                        }
                    })
                    .filter(Objects::nonNull)
                    .sorted(Comparator.comparingInt(Chunk::getSequence))  // ‚úÖ ÊåâÂ∫èÂè∑ÊéíÂ∫è
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to get chunks for document: {}", documentId, e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteChunk(String chunkId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", chunkId)).first();
            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("Deleted chunk: {}", chunkId);
            }
        } catch (Exception e) {
            log.error("Failed to delete chunk: {}", chunkId, e);
        }
    }

    @Override
    public void deleteChunksByDocument(String documentId) {
        try {
            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®ÊâπÈáèÂà†Èô§Êõø‰ª£ÈÄê‰∏™Âà†Èô§Ôºà50-100ÂÄçÊèêÂçáÔºâ
            Query query = new Query(Criteria
                    .where("metadata.documentId").is(documentId)
                    .and("metadata.type").is("chunk"));

            DeleteResult result = mongoTemplate.remove(query, "fs.files");

            // ÂêåÊó∂Âà†Èô§ÂØπÂ∫îÁöÑchunksÊï∞ÊçÆ
            mongoTemplate.remove(query, "fs.chunks");

            log.info("‚úÖ Deleted {} chunks for document: {}", result.getDeletedCount(), documentId);
        } catch (Exception e) {
            log.error("Failed to delete chunks for document: {}", documentId, e);
        }
    }

    // ========== Image Storage ==========

    @Override
    public String saveImage(String documentId, Image image) {
        try {
            // ‚≠ê Âº∫Âà∂Ë¶ÅÊ±ÇÈ°µÁ†Å‰ø°ÊÅØ
            Integer pageNum = image.getPageNumber();
            if (pageNum == null || pageNum <= 0) {
                throw new IllegalArgumentException(
                    String.format("Image must have valid pageNumber (got: %s, documentId: %s). " +
                                "All images must be assigned a page number.",
                                pageNum, documentId));
            }

            // ‰ªé metadata ‰∏≠Ëé∑ÂèñÂõæÁâáÂ∫èÂè∑ÂíåÂü∫Á°ÄÊñá‰ª∂Âêç
            Integer imageIndex = null;
            String baseName = documentId;  // ÈªòËÆ§‰ΩøÁî®documentId
            if (image.getMetadata() != null) {
                if (image.getMetadata().containsKey("imageIndex")) {
                    imageIndex = ((Number) image.getMetadata().get("imageIndex")).intValue();
                }
                if (image.getMetadata().containsKey("baseName")) {
                    baseName = (String) image.getMetadata().get("baseName");
                }
            }

            // ‚≠ê ÁîüÊàêÁÆÄÊ¥ÅÁöÑÂõæÁâáIDÔºöbaseName_p001_i000
            String imageId = String.format("%s_p%03d_i%03d",
                    baseName, pageNum, imageIndex != null ? imageIndex : 0);

            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("imageId", imageId)
                    .append("type", "image")
                    .append("format", image.getFormat())
                    .append("pageNumber", pageNum)
                    .append("baseName", baseName);

            if (imageIndex != null) {
                metadata.append("imageIndex", imageIndex);
            }

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] data = objectMapper.writeValueAsBytes(image);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    imageId,
                    new ByteArrayInputStream(data),
                    options
            );

            log.debug("Saved image: {} with GridFS ID: {}", imageId, fileId);
            return imageId;
        } catch (Exception e) {
            log.error("Failed to save image", e);
            return null;
        }
    }

    @Override
    public Optional<Image> getImage(String imageId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", imageId)).first();
            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            Image image = objectMapper.readValue(outputStream.toByteArray(), Image.class);
            return Optional.of(image);
        } catch (Exception e) {
            log.error("Failed to get image: {}", imageId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<Image> getImagesByDocument(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
                            .append("metadata.type", "image")
            ).into(new ArrayList<>());

            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®Âπ∂Ë°åÊµÅÂä†ÈÄü‰∏ãËΩΩÔºà4-8ÂÄçÊèêÂçáÔºâ
            return files.parallelStream()
                    .map(file -> {
                        try {
                            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
                            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
                            return objectMapper.readValue(outputStream.toByteArray(), Image.class);
                        } catch (Exception e) {
                            log.error("Failed to read image file", e);
                            return null;
                        }
                    })
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to get images for document: {}", documentId, e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteImage(String imageId) {
        try {
            GridFSFile file = gridFSBucket.find(new Document("filename", imageId)).first();
            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.debug("Deleted image: {}", imageId);
            }
        } catch (Exception e) {
            log.error("Failed to delete image: {}", imageId, e);
        }
    }

    @Override
    public void deleteImagesByDocument(String documentId) {
        try {
            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®ÊâπÈáèÂà†Èô§Êõø‰ª£ÈÄê‰∏™Âà†Èô§Ôºà50-100ÂÄçÊèêÂçáÔºâ
            Query query = new Query(Criteria
                    .where("metadata.documentId").is(documentId)
                    .and("metadata.type").is("image"));

            DeleteResult result = mongoTemplate.remove(query, "fs.files");

            // ÂêåÊó∂Âà†Èô§ÂØπÂ∫îÁöÑchunksÊï∞ÊçÆ
            mongoTemplate.remove(query, "fs.chunks");

            log.info("‚úÖ Deleted {} images for document: {}", result.getDeletedCount(), documentId);
        } catch (Exception e) {
            log.error("Failed to delete images for document: {}", documentId, e);
        }
    }

    /**
     * ÈÄöËøáÂìàÂ∏åÂÄºÊü•ÊâæÂõæÁâáÔºàÁî®‰∫éÂéªÈáçÔºâ‚≠ê NEW
     */
    @Override
    public Optional<String> findImageByHash(String imageHash) {
        try {
            // Êü•Êâæ metadata.imageHash ÂåπÈÖçÁöÑÊñá‰ª∂
            GridFSFile file = gridFSBucket.find(
                    new Document("metadata.type", "image")
                            .append("metadata.imageHash", imageHash)
            ).first();

            if (file != null && file.getMetadata() != null) {
                String imageId = file.getMetadata().getString("imageId");
                if (imageId != null) {
                    log.debug("üîç ÊâæÂà∞ÈáçÂ§çÂõæÁâá: hash={}, imageId={}",
                            imageHash.substring(0, Math.min(16, imageHash.length())), imageId);
                    return Optional.of(imageId);
                }
            }

            return Optional.empty();
        } catch (Exception e) {
            log.error("Failed to find image by hash", e);
            return Optional.empty();
        }
    }

    // ========== PPL Data Storage ==========

    @Override
    public String savePPLData(String documentId, PPLData data) {
        try {
            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("type", "ppl");

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] jsonData = objectMapper.writeValueAsBytes(data);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    documentId + "_ppl",
                    new ByteArrayInputStream(jsonData),
                    options
            );

            log.debug("Saved PPL data for document: {} with GridFS ID: {}", documentId, fileId);
            return documentId;
        } catch (Exception e) {
            log.error("Failed to save PPL data", e);
            return null;
        }
    }

    @Override
    public Optional<PPLData> getPPLData(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", documentId + "_ppl")
            ).first();

            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            PPLData pplData = objectMapper.readValue(outputStream.toByteArray(), PPLData.class);
            return Optional.of(pplData);
        } catch (Exception e) {
            log.error("Failed to get PPL data for document: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public void deletePPLData(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", documentId + "_ppl")
            ).first();

            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.info("Deleted PPL data for document: {}", documentId);
            }
        } catch (Exception e) {
            log.error("Failed to delete PPL data for document: {}", documentId, e);
        }
    }

    // ========== Optimization Data Storage ==========

    @Override
    public String saveOptimizationData(String documentId, OptimizationData data) {
        try {
            String filename = documentId + "_opt_" + data.getOptimizationType();

            Document metadata = new Document()
                    .append("documentId", documentId)
                    .append("optimizationType", data.getOptimizationType())
                    .append("type", "optimization");

            GridFSUploadOptions options = new GridFSUploadOptions()
                    .metadata(metadata);

            byte[] jsonData = objectMapper.writeValueAsBytes(data);
            ObjectId fileId = gridFSBucket.uploadFromStream(
                    filename,
                    new ByteArrayInputStream(jsonData),
                    options
            );

            log.debug("Saved {} optimization data for document: {} with GridFS ID: {}",
                     data.getOptimizationType(), documentId, fileId);
            return documentId + ":" + data.getOptimizationType();
        } catch (Exception e) {
            log.error("Failed to save optimization data", e);
            return null;
        }
    }

    @Override
    public Optional<OptimizationData> getOptimizationData(String documentId, String optimizationType) {
        try {
            String filename = documentId + "_opt_" + optimizationType;
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", filename)
            ).first();

            if (file == null) {
                return Optional.empty();
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);

            OptimizationData optData =
                objectMapper.readValue(outputStream.toByteArray(),
                                     OptimizationData.class);
            return Optional.of(optData);
        } catch (Exception e) {
            log.error("Failed to get {} optimization data for document: {}", optimizationType, documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<OptimizationData> getAllOptimizationData(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
                            .append("metadata.type", "optimization")
            ).into(new ArrayList<>());

            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®Âπ∂Ë°åÊµÅÂä†ÈÄü‰∏ãËΩΩ
            return files.parallelStream()
                    .map(file -> {
                        try {
                            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
                            gridFSBucket.downloadToStream(file.getObjectId(), outputStream);
                            return objectMapper.readValue(outputStream.toByteArray(),
                                                        OptimizationData.class);
                        } catch (Exception e) {
                            log.error("Failed to read optimization data file", e);
                            return null;
                        }
                    })
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to get all optimization data for document: {}", documentId, e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteOptimizationData(String documentId, String optimizationType) {
        try {
            String filename = documentId + "_opt_" + optimizationType;
            GridFSFile file = gridFSBucket.find(
                    new Document("filename", filename)
            ).first();

            if (file != null) {
                gridFSBucket.delete(file.getObjectId());
                log.info("Deleted {} optimization data for document: {}", optimizationType, documentId);
            }
        } catch (Exception e) {
            log.error("Failed to delete {} optimization data for document: {}", optimizationType, documentId, e);
        }
    }

    @Override
    public void deleteAllOptimizationData(String documentId) {
        try {
            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®ÊâπÈáèÂà†Èô§Êõø‰ª£ÈÄê‰∏™Âà†Èô§Ôºà50-100ÂÄçÊèêÂçáÔºâ
            Query query = new Query(Criteria
                    .where("metadata.documentId").is(documentId)
                    .and("metadata.type").is("optimization"));

            DeleteResult result = mongoTemplate.remove(query, "fs.files");

            // ÂêåÊó∂Âà†Èô§ÂØπÂ∫îÁöÑchunksÊï∞ÊçÆ
            mongoTemplate.remove(query, "fs.chunks");

            log.info("‚úÖ Deleted {} optimization data items for document: {}", result.getDeletedCount(), documentId);
        } catch (Exception e) {
            log.error("Failed to delete all optimization data for document: {}", documentId, e);
        }
    }

    // ========== Document Management ==========

    @Override
    public List<DocumentMetadata> listAllDocuments() {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).into(new ArrayList<>());

            return files.stream()
                    .map(this::convertToDocumentMetadata)
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to list all documents", e);
            return new ArrayList<>();
        }
    }

    @Override
    public List<DocumentMetadata> listDocuments(int offset, int limit) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).skip(offset).limit(limit).into(new ArrayList<>());

            return files.stream()
                    .map(this::convertToDocumentMetadata)
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to list documents with pagination", e);
            return new ArrayList<>();
        }
    }

    @Override
    public List<DocumentMetadata> searchDocuments(String keyword) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).into(new ArrayList<>());

            return files.stream()
                    .filter(file -> {
                        String filename = file.getMetadata() != null ?
                                file.getMetadata().getString("filename") : "";
                        return filename != null && filename.contains(keyword);
                    })
                    .map(this::convertToDocumentMetadata)
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Failed to search documents with keyword: {}", keyword, e);
            return new ArrayList<>();
        }
    }

    @Override
    public long getDocumentCount() {
        try {
            return gridFSBucket.find(
                    new Document("metadata.type", "document")
            ).into(new ArrayList<>()).size();
        } catch (Exception e) {
            log.error("Failed to get document count", e);
            return 0;
        }
    }

    private DocumentMetadata convertToDocumentMetadata(GridFSFile file) {
        try {
            Document metadata = file.getMetadata();
            if (metadata == null) {
                return null;
            }

            return DocumentMetadata.builder()
                    .documentId(metadata.getString("documentId"))
                    .filename(metadata.getString("filename"))
                    .fileSize(file.getLength())
                    .uploadTime(file.getUploadDate())
                    .lastModified(file.getUploadDate())
                    .build();
        } catch (Exception e) {
            log.error("Failed to convert GridFSFile to DocumentMetadata", e);
            return null;
        }
    }

    @Override
    public void cleanupDocument(String documentId) {
        deleteDocument(documentId);
        deleteChunksByDocument(documentId);
        deleteImagesByDocument(documentId);
        deletePPLData(documentId);
        deleteAllOptimizationData(documentId);
        deleteExtractedText(documentId);  // ‚≠ê Êñ∞Â¢û
        log.info("Cleaned up all data for document: {}", documentId);
    }

    @Override
    public boolean documentExists(String documentId) {
        try {
            GridFSFile file = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
            ).first();
            return file != null;
        } catch (Exception e) {
            log.error("Failed to check document existence: {}", documentId, e);
            return false;
        }
    }

    @Override
    public long getDocumentSize(String documentId) {
        try {
            List<GridFSFile> files = gridFSBucket.find(
                    new Document("metadata.documentId", documentId)
            ).into(new ArrayList<>());

            return files.stream()
                    .mapToLong(GridFSFile::getLength)
                    .sum();
        } catch (Exception e) {
            log.error("Failed to calculate document size for: {}", documentId, e);
            return 0;
        }
    }

    // ========== Statistics ==========

    @Override
    public StorageStatistics getStatistics() {
        try {
            // ‚úÖ P0‰ºòÂåñÔºö‰ΩøÁî®MongoDBËÅöÂêàÁÆ°ÈÅìÔºå‰∏ÄÊ¨°Êü•ËØ¢Ëé∑ÂèñÊâÄÊúâÁªüËÆ°
            // ÊÄßËÉΩÊèêÂçáÔºö100-1000ÂÄçÔºàÈÅøÂÖçÂÖ®Ë°®Êâ´ÊèèÂíåÂä†ËΩΩÊâÄÊúâÊñá‰ª∂Âà∞ÂÜÖÂ≠òÔºâ
            String collection = "fs.files";

            Aggregation aggregation = Aggregation.newAggregation(
                // ÊåâÁ±ªÂûãÂàÜÁªÑÁªüËÆ°
                Aggregation.group("metadata.type")
                    .count().as("count")
                    .sum("length").as("totalSize")
                    .addToSet("metadata.documentId").as("documentIds"),

                // ÊäïÂΩ±ÁªìÊûú
                Aggregation.project("count", "totalSize", "documentIds")
                    .and("_id").as("type")
            );

            AggregationResults<Document> results = mongoTemplate.aggregate(
                aggregation, collection, Document.class
            );

            // Ëß£ÊûêËÅöÂêàÁªìÊûú
            long totalChunks = 0, totalImages = 0, totalPPLData = 0, totalSize = 0;
            Set<String> allDocumentIds = new HashSet<>();

            for (Document doc : results.getMappedResults()) {
                String type = doc.getString("type");
                Number countNum = (Number) doc.get("count");
                Number sizeNum = (Number) doc.get("totalSize");

                long count = countNum != null ? countNum.longValue() : 0;
                long size = sizeNum != null ? sizeNum.longValue() : 0;

                @SuppressWarnings("unchecked")
                List<String> docIds = (List<String>) doc.get("documentIds");

                if (docIds != null) {
                    allDocumentIds.addAll(docIds);
                }

                if (type != null) {
                    switch (type) {
                        case "chunk" -> totalChunks = count;
                        case "image" -> totalImages = count;
                        case "ppl" -> totalPPLData = count;
                    }
                }
                totalSize += size;
            }

            log.debug("‚úÖ Statistics calculated using aggregation: {} docs, {} chunks, {} images",
                    allDocumentIds.size(), totalChunks, totalImages);

            return StorageStatistics.builder()
                    .totalDocuments(allDocumentIds.size())
                    .totalChunks(totalChunks)
                    .totalImages(totalImages)
                    .totalPPLData(totalPPLData)
                    .totalSize(totalSize)
                    .storageType("mongodb-gridfs")
                    .healthy(isHealthy())
                    .timestamp(System.currentTimeMillis())
                    .build();

        } catch (Exception e) {
            log.error("Failed to get statistics", e);
            return StorageStatistics.builder()
                    .storageType("mongodb-gridfs")
                    .healthy(false)
                    .timestamp(System.currentTimeMillis())
                    .build();
        }
    }

    @Override
    public boolean isHealthy() {
        try {
            // ÊµãËØï MongoDB ËøûÊé•
            mongoTemplate.getDb().listCollectionNames().first();
            return true;
        } catch (Exception e) {
            log.error("Health check failed", e);
            return false;
        }
    }

    // ========== Êñá‰ª∂Á≥ªÁªüÊµèËßàÂÆûÁé∞ (File System Browse Implementation) ==========
    // MongoDBÈÄöËøáGridFSÂíåÊñáÊ°£ÁöÑpathÂ≠óÊÆµÂÆûÁé∞ËôöÊãüÊñá‰ª∂Á≥ªÁªü

    @Override
    public List<Map<String, Object>> listFiles(String virtualPath) {
        try {
            List<Map<String, Object>> items = new ArrayList<>();
            String searchPath = virtualPath.isEmpty() ? "" : virtualPath + "/";

            // ‰ΩøÁî®GridFSBucketÊü•ËØ¢Êñá‰ª∂
            Document query = new Document();
            if (!searchPath.isEmpty()) {
                query.append("metadata.path", new Document("$regex", "^" + searchPath));
            }

            List<GridFSFile> files = gridFSBucket.find(query).into(new ArrayList<>());
            Set<String> directories = new HashSet<>();

            for (GridFSFile gridFSFile : files) {
                Document metadata = gridFSFile.getMetadata();
                String path = metadata != null ? metadata.getString("path") : "";

                if (path != null && path.startsWith(searchPath)) {
                    String relativePath = path.substring(searchPath.length());
                    int slashIndex = relativePath.indexOf('/');

                    if (slashIndex > 0) {
                        // Â≠êÁõÆÂΩï
                        String dirName = relativePath.substring(0, slashIndex);
                        if (!directories.contains(dirName)) {
                            directories.add(dirName);
                            Map<String, Object> dirItem = new HashMap<>();
                            dirItem.put("name", dirName);
                            dirItem.put("type", "directory");
                            dirItem.put("path", virtualPath.isEmpty() ? dirName : virtualPath + "/" + dirName);
                            items.add(dirItem);
                        }
                    } else {
                        // Êñá‰ª∂
                        Map<String, Object> fileItem = new HashMap<>();
                        fileItem.put("name", relativePath);
                        fileItem.put("type", "file");
                        fileItem.put("path", path);
                        fileItem.put("size", gridFSFile.getLength());
                        fileItem.put("modified", gridFSFile.getUploadDate().getTime());
                        items.add(fileItem);
                    }
                }
            }

            return items;
        } catch (Exception e) {
            log.error("ÂàóÂá∫Êñá‰ª∂Â§±Ë¥•: {}", virtualPath, e);
            throw new RuntimeException("ÂàóÂá∫Êñá‰ª∂Â§±Ë¥•: " + e.getMessage(), e);
        }
    }

    @Override
    public byte[] readFile(String virtualPath) {
        try {
            Document query = new Document("metadata.path", virtualPath);
            GridFSFile gridFSFile = gridFSBucket.find(query).first();

            if (gridFSFile == null) {
                log.warn("Êñá‰ª∂‰∏çÂ≠òÂú®: {}", virtualPath);
                return null;
            }

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            gridFSBucket.downloadToStream(gridFSFile.getObjectId(), outputStream);
            return outputStream.toByteArray();
        } catch (Exception e) {
            log.error("ËØªÂèñÊñá‰ª∂Â§±Ë¥•: {}", virtualPath, e);
            throw new RuntimeException("ËØªÂèñÊñá‰ª∂Â§±Ë¥•: " + e.getMessage(), e);
        }
    }

    @Override
    public boolean deleteFile(String virtualPath) {
        try {
            // Âà†Èô§Êñá‰ª∂ÊàñÊï¥‰∏™ÁõÆÂΩï
            Document query = new Document("metadata.path", new Document("$regex", "^" + virtualPath));

            List<GridFSFile> files = gridFSBucket.find(query).into(new ArrayList<>());
            for (GridFSFile file : files) {
                gridFSBucket.delete(file.getObjectId());
            }

            log.info("‚úÖ Âà†Èô§ÊàêÂäü: {}", virtualPath);
            return !files.isEmpty();
        } catch (Exception e) {
            log.error("Âà†Èô§Â§±Ë¥•: {}", virtualPath, e);
            return false;
        }
    }

    @Override
    public boolean createDirectory(String virtualPath) {
        try {
            // MongoDB GridFS‰∏çÈúÄË¶ÅÊòæÂºèÂàõÂª∫ÁõÆÂΩï
            // ÂàõÂª∫‰∏Ä‰∏™Ê†áËÆ∞ÊñáÊ°£
            Document metadata = new Document();
            metadata.put("path", virtualPath);
            metadata.put("type", "directory");
            metadata.put("created", System.currentTimeMillis());

            GridFSUploadOptions options = new GridFSUploadOptions().metadata(metadata);

            gridFSBucket.uploadFromStream(
                virtualPath + "/.dir",
                new ByteArrayInputStream(new byte[0]),
                options
            );

            log.info("‚úÖ ÂàõÂª∫ÁõÆÂΩïÊàêÂäü: {}", virtualPath);
            return true;
        } catch (Exception e) {
            log.error("ÂàõÂª∫ÁõÆÂΩïÂ§±Ë¥•: {}", virtualPath, e);
            return false;
        }
    }

    @Override
    public Map<String, Object> getStorageStats(String virtualPath) {
        try {
            String searchPath = virtualPath.isEmpty() ? "" : virtualPath + "/";
            Document query = new Document();

            if (!searchPath.isEmpty()) {
                query.append("metadata.path", new Document("$regex", "^" + searchPath));
            }

            long[] stats = {0, 0, 0}; // [files, folders, size]

            List<GridFSFile> files = gridFSBucket.find(query).into(new ArrayList<>());
            for (GridFSFile gridFSFile : files) {
                Document metadata = gridFSFile.getMetadata();
                String type = metadata != null ? metadata.getString("type") : "file";

                if ("directory".equals(type)) {
                    stats[1]++;
                } else {
                    stats[0]++;
                    stats[2] += gridFSFile.getLength();
                }
            }

            return Map.of(
                "totalFiles", stats[0],
                "totalFolders", stats[1],
                "totalSize", stats[2]
            );
        } catch (Exception e) {
            log.error("Ëé∑ÂèñÂ≠òÂÇ®ÁªüËÆ°Â§±Ë¥•: {}", virtualPath, e);
            return Map.of(
                "totalFiles", 0L,
                "totalFolders", 0L,
                "totalSize", 0L
            );
        }
    }

    // ========== ‰∫ãÂä°ÊÄßÊâπÈáèÊìç‰Ωú ‚≠ê NEW ==========

    @Override
    public BatchOperationResult saveDocumentsTransactional(List<Map<String, Object>> documents)
            throws BatchOperationException {

        List<String> successIds = new ArrayList<>();
        Map<String, String> errorMessages = new HashMap<>();

        try {
            for (Map<String, Object> doc : documents) {
                String documentId = (String) doc.get("documentId");
                String filename = (String) doc.get("filename");
                byte[] fileData = (byte[]) doc.get("fileData");

                try {
                    String id = saveDocument(documentId, filename, fileData);
                    if (id != null) {
                        successIds.add(id);
                    } else {
                        throw new StorageException("SAVE_FAILED", documentId, "Failed to save");
                    }
                } catch (Exception e) {
                    errorMessages.put(documentId, e.getMessage());
                    throw e;
                }
            }

            log.info("‚úÖ Transaction: All {} documents saved successfully", successIds.size());
            return BatchOperationResult.builder()
                    .successCount(successIds.size())
                    .failureCount(0)
                    .totalCount(documents.size())
                    .successIds(successIds)
                    .failureIds(new ArrayList<>())
                    .errorMessages(new HashMap<>())
                    .build();

        } catch (Exception e) {
            log.warn("‚èÆ Transaction failed, rolling back {} documents...", successIds.size());

            for (String docId : successIds) {
                try {
                    deleteDocument(docId);
                    log.debug("  ‚Ü© Rolled back: {}", docId);
                } catch (Exception rollbackError) {
                    log.error("  ‚ùå Rollback failed: {}", docId, rollbackError);
                    errorMessages.put(docId, "Rollback failed: " + rollbackError.getMessage());
                }
            }

            throw new BatchOperationException(
                "Batch save operation failed and rolled back: " + e.getMessage(),
                e, new ArrayList<>(), successIds, errorMessages
            );
        }
    }

    @Override
    public BatchOperationResult deleteDocumentsTransactional(List<String> documentIds)
            throws BatchOperationException {

        Map<String, byte[]> backups = new HashMap<>();
        List<String> successIds = new ArrayList<>();
        Map<String, String> errorMessages = new HashMap<>();

        try {
            // Â§á‰ªΩÈò∂ÊÆµ
            log.debug("üì¶ Phase 1: Backing up {} documents...", documentIds.size());
            for (String documentId : documentIds) {
                try {
                    Optional<byte[]> data = getDocument(documentId);
                    if (data.isPresent()) {
                        backups.put(documentId, data.get());
                        log.debug("  ‚úì Backed up: {}", documentId);
                    }
                } catch (Exception e) {
                    errorMessages.put(documentId, "Backup failed: " + e.getMessage());
                    throw e;
                }
            }

            // Âà†Èô§Èò∂ÊÆµ
            log.debug("üóëÔ∏è Phase 2: Deleting {} documents...", documentIds.size());
            for (String documentId : documentIds) {
                try {
                    if (backups.containsKey(documentId)) {
                        deleteDocument(documentId);
                        successIds.add(documentId);
                        log.debug("  ‚úì Deleted: {}", documentId);
                    }
                } catch (Exception e) {
                    errorMessages.put(documentId, "Delete failed: " + e.getMessage());
                    throw e;
                }
            }

            log.info("‚úÖ Transaction: All {} documents deleted successfully", successIds.size());
            return BatchOperationResult.builder()
                    .successCount(successIds.size())
                    .failureCount(0)
                    .totalCount(documentIds.size())
                    .successIds(successIds)
                    .failureIds(new ArrayList<>())
                    .errorMessages(new HashMap<>())
                    .build();

        } catch (Exception e) {
            log.warn("‚èÆ Transaction failed, restoring {} documents...", successIds.size());

            for (String docId : successIds) {
                try {
                    byte[] data = backups.get(docId);
                    if (data != null) {
                        saveDocument(docId, docId, data);
                        log.debug("  ‚Ü© Restored: {}", docId);
                    }
                } catch (Exception restoreError) {
                    log.error("  ‚ùå Restore failed: {}", docId, restoreError);
                    errorMessages.put(docId, "Restore failed: " + restoreError.getMessage());
                }
            }

            throw new BatchOperationException(
                "Batch delete operation failed and restored: " + e.getMessage(),
                e, new ArrayList<>(), successIds, errorMessages
            );
        }
    }

    // ========== ÂÖÉÊï∞ÊçÆÁÆ°ÁêÜ ‚≠ê NEW ==========

    @Override
    public void saveMetadata(DocumentMetadata metadata) {
        try {
            mongoTemplate.save(metadata, "document_metadata");
            log.debug("üíæ Saved metadata: {}", metadata.getDocumentId());
        } catch (Exception e) {
            log.error("Failed to save metadata: {}", metadata.getDocumentId(), e);
        }
    }

    @Override
    public Optional<DocumentMetadata> getMetadata(String documentId) {
        try {
            DocumentMetadata metadata = mongoTemplate.findById(documentId, DocumentMetadata.class, "document_metadata");
            return Optional.ofNullable(metadata);
        } catch (Exception e) {
            log.error("Failed to get metadata: {}", documentId, e);
            return Optional.empty();
        }
    }

    @Override
    public List<DocumentMetadata> getAllMetadata() {
        try {
            return mongoTemplate.findAll(DocumentMetadata.class, "document_metadata");
        } catch (Exception e) {
            log.error("Failed to get all metadata", e);
            return new ArrayList<>();
        }
    }

    @Override
    public void deleteMetadata(String documentId) {
        try {
            mongoTemplate.remove(
                new org.springframework.data.mongodb.core.query.Query(
                    org.springframework.data.mongodb.core.query.Criteria.where("_id").is(documentId)
                ),
                "document_metadata"
            );
            log.debug("üóëÔ∏è Deleted metadata: {}", documentId);
        } catch (Exception e) {
            log.error("Failed to delete metadata: {}", documentId, e);
        }
    }
}
