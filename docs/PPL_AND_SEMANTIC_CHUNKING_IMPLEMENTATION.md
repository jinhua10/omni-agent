# ğŸ¯ PPL å’Œ Semantic åˆ†å—ç­–ç•¥å®ç°è¯´æ˜

**å®ç°æ—¥æœŸ**: 2025-12-18  
**ç‰ˆæœ¬**: v1.0

---

## ğŸ“‹ å®ç°æ¦‚è¿°

### âœ… å·²å®ç°çš„ç­–ç•¥

| ç­–ç•¥ | ç±»å | çŠ¶æ€ | ç²¾åº¦ | é€Ÿåº¦ |
|------|------|------|------|------|
| **PPL å›°æƒ‘åº¦åˆ†å—** | `PPLChunkingStrategy` | âœ… | â­â­â­â­â­ | â­â­â­ |
| **è¯­ä¹‰åˆ†å—** | `SemanticChunkingStrategy` | âœ… | â­â­â­â­â­ | â­â­â­ |

---

## ğŸ” PPL å›°æƒ‘åº¦åˆ†å—ç­–ç•¥

### åŸç†

**PPL (Probable Point of Loss)** - åŸºäºå›°æƒ‘åº¦çš„åˆ†å—

**æ ¸å¿ƒæ€æƒ³**:
- å›°æƒ‘åº¦ = è¯­è¨€æ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªè¯çš„ä¸ç¡®å®šæ€§
- **é«˜å›°æƒ‘åº¦** = ä¸»é¢˜è½¬æ¢ç‚¹ = åˆ†å—è¾¹ç•Œ
- **ä½å›°æƒ‘åº¦** = ä¸»é¢˜å»¶ç»­ = åŒä¸€åˆ†å—

### ç®€åŒ–å®ç°ï¼ˆä¸ä¾èµ–è¯­è¨€æ¨¡å‹ï¼‰

```java
// ä½¿ç”¨è¯æ±‡é‡å åº¦ä½œä¸ºå›°æƒ‘åº¦çš„è¿‘ä¼¼
double overlap = calculateWordOverlap(sentence1, sentence2);
double perplexity = 1.0 - overlap;  // é‡å åº¦ä½ = å›°æƒ‘åº¦é«˜
```

**ä¼˜åŠ¿**:
- âœ… æ— éœ€è¯­è¨€æ¨¡å‹ï¼Œé€Ÿåº¦å¿«
- âœ… åœ¨ä¸»é¢˜è½¬æ¢å¤„åˆ‡åˆ†
- âœ… ä¿æŒè¯­ä¹‰å®Œæ•´æ€§

**é€‚ç”¨åœºæ™¯**:
- API æ–‡æ¡£ï¼ˆæ£€æµ‹æ¥å£è¾¹ç•Œï¼‰
- é•¿ç¯‡æ–‡ç« ï¼ˆæ£€æµ‹ä¸»é¢˜è½¬æ¢ï¼‰
- å¤šä¸»é¢˜æ–‡æ¡£

### å®ç°ç»†èŠ‚

```java
@Component
public class PPLChunkingStrategy implements ChunkingStrategy {
    
    @Override
    public List<Chunk> chunk(String documentId, String content, 
                            Map<String, Object> params) {
        // 1. æŒ‰å¥å­åˆ†å‰²
        List<Sentence> sentences = splitIntoSentences(content);
        
        // 2. è®¡ç®—å¥å­é—´çš„"å›°æƒ‘åº¦"
        //    ä½¿ç”¨ Jaccard ç›¸ä¼¼åº¦çš„å€’æ•°
        List<Double> perplexities = calculatePerplexities(sentences);
        
        // 3. æ‰¾åˆ°å›°æƒ‘åº¦å³°å€¼ç‚¹ï¼ˆå±€éƒ¨æœ€å¤§å€¼ï¼‰
        List<Integer> boundaries = findBoundaries(perplexities, ...);
        
        // 4. åœ¨å³°å€¼ç‚¹åˆ‡åˆ†
        return createChunks(boundaries, sentences);
    }
    
    // è®¡ç®—ä¸¤ä¸ªå¥å­çš„è¯æ±‡é‡å åº¦
    private double calculateWordOverlap(String sent1, String sent2) {
        Set<String> words1 = tokenize(sent1);
        Set<String> words2 = tokenize(sent2);
        
        // Jaccard ç›¸ä¼¼åº¦ = |äº¤é›†| / |å¹¶é›†|
        return intersection(words1, words2).size() / 
               union(words1, words2).size();
    }
    
    // åˆ¤æ–­æ˜¯å¦æ˜¯å›°æƒ‘åº¦å³°å€¼
    private boolean isPeakPoint(List<Double> perplexities, int index) {
        // å±€éƒ¨æœ€å¤§å€¼ && è¶…è¿‡é˜ˆå€¼
        return perplexities.get(index) > perplexities.get(index-1) &&
               perplexities.get(index) > perplexities.get(index+1) &&
               perplexities.get(index) > threshold;
    }
}
```

### å‚æ•°é…ç½®

```java
Map<String, Object> params = Map.of(
    "minChunkSize", 200,    // æœ€å°åˆ†å—å¤§å°
    "maxChunkSize", 800,    // æœ€å¤§åˆ†å—å¤§å°
    "threshold", 0.3        // å›°æƒ‘åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
);
```

### ä½¿ç”¨ç¤ºä¾‹

```java
// è‡ªåŠ¨é€‰æ‹©ï¼ˆAPIæ–‡æ¡£æˆ–é•¿æ–‡ç« ä¼šè‡ªåŠ¨ä½¿ç”¨PPLï¼‰
List<Chunk> chunks = chunkingService.chunkDocument(
    "doc_123_api.yaml", 
    apiContent,
    "api.yaml"
);

// æ‰‹åŠ¨æŒ‡å®š
List<Chunk> chunks = strategyManager.chunkWithStrategy(
    documentId, content, "ppl", params
);
```

### æ•ˆæœç¤ºä¾‹

**è¾“å…¥æ–‡æ¡£**:
```
æ¥å£1ï¼šåˆ›å»ºç”¨æˆ·
POST /api/users
å‚æ•°ï¼šname, email

æ¥å£2ï¼šè·å–ç”¨æˆ·
GET /api/users/{id}
è¿”å›ï¼šç”¨æˆ·ä¿¡æ¯
```

**åˆ†å—ç»“æœ**:
```
Chunk 1: [å›°æƒ‘åº¦ä½ â†’ åŒä¸€ä¸»é¢˜]
  æ¥å£1ï¼šåˆ›å»ºç”¨æˆ·
  POST /api/users
  å‚æ•°ï¼šname, email

Chunk 2: [å›°æƒ‘åº¦å³°å€¼ â†’ ä¸»é¢˜è½¬æ¢]
  æ¥å£2ï¼šè·å–ç”¨æˆ·
  GET /api/users/{id}
  è¿”å›ï¼šç”¨æˆ·ä¿¡æ¯
```

---

## ğŸ§  è¯­ä¹‰åˆ†å—ç­–ç•¥

### åŸç†

åŸºäº**æ®µè½è¯­ä¹‰ç›¸ä¼¼åº¦**çš„æ™ºèƒ½åˆ†å—

**æ ¸å¿ƒæ€æƒ³**:
- è®¡ç®—ç›¸é‚»æ®µè½çš„è¯­ä¹‰ç›¸ä¼¼åº¦
- **ç›¸ä¼¼åº¦é«˜** = åŒä¸€ä¸»é¢˜ = åˆå¹¶åˆ°åŒä¸€åˆ†å—
- **ç›¸ä¼¼åº¦ä½** = ä¸»é¢˜è½¬æ¢ = åˆ†å—è¾¹ç•Œ

### ç®€åŒ–å®ç°ï¼ˆä¸ä¾èµ–å‘é‡æ¨¡å‹ï¼‰

```java
// ä½¿ç”¨ TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦
Map<String, Integer> vec1 = calculateWordVector(paragraph1);
Map<String, Integer> vec2 = calculateWordVector(paragraph2);

double similarity = cosineSimilarity(vec1, vec2);
// ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ ä¸»é¢˜è½¬æ¢ â†’ åˆ‡åˆ†
```

**ä¼˜åŠ¿**:
- âœ… æ— éœ€å‘é‡æ¨¡å‹ï¼Œé€Ÿåº¦å¿«
- âœ… ä¿æŒä¸»é¢˜è¿è´¯æ€§
- âœ… é€‚åˆæŠ€æœ¯æ–‡æ¡£å’Œä»£ç 

**é€‚ç”¨åœºæ™¯**:
- æŠ€æœ¯æ–‡æ¡£ï¼ˆä¿æŒä»£ç ç¤ºä¾‹å®Œæ•´ï¼‰
- ä»£ç åº“ï¼ˆä¿æŒå‡½æ•°é€»è¾‘å®Œæ•´ï¼‰
- å¤šä¸»é¢˜æ–‡æ¡£

### å®ç°ç»†èŠ‚

```java
@Component
public class SemanticChunkingStrategy implements ChunkingStrategy {
    
    @Override
    public List<Chunk> chunk(String documentId, String content, 
                            Map<String, Object> params) {
        // 1. æŒ‰æ®µè½åˆ†å‰²
        List<Paragraph> paragraphs = splitIntoParagraphs(content);
        
        // 2. è®¡ç®—è¯é¢‘å‘é‡ï¼ˆç®€åŒ–çš„ TF-IDFï¼‰
        List<Map<String, Integer>> vectors = calculateWordVectors(paragraphs);
        
        // 3. è®¡ç®—ç›¸é‚»æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦
        List<Double> similarities = calculateSimilarities(vectors);
        
        // 4. åœ¨ç›¸ä¼¼åº¦ä½çš„ä½ç½®åˆ‡åˆ†
        List<Integer> boundaries = findSemanticBoundaries(similarities, ...);
        
        return createChunks(boundaries, paragraphs);
    }
    
    // è®¡ç®—è¯é¢‘å‘é‡
    private Map<String, Integer> calculateWordVector(String paragraph) {
        Map<String, Integer> wordCount = new HashMap<>();
        String[] words = paragraph.toLowerCase().split("\\s+");
        
        for (String word : words) {
            wordCount.merge(word, 1, Integer::sum);
        }
        
        return wordCount;
    }
    
    // ä½™å¼¦ç›¸ä¼¼åº¦
    private double cosineSimilarity(Map<String, Integer> vec1, 
                                   Map<String, Integer> vec2) {
        // ç‚¹ç§¯
        double dotProduct = 0.0;
        for (String key : vec1.keySet()) {
            if (vec2.containsKey(key)) {
                dotProduct += vec1.get(key) * vec2.get(key);
            }
        }
        
        // å‘é‡é•¿åº¦
        double magnitude1 = Math.sqrt(vec1.values().stream()
            .mapToDouble(v -> v * v).sum());
        double magnitude2 = Math.sqrt(vec2.values().stream()
            .mapToDouble(v -> v * v).sum());
        
        return dotProduct / (magnitude1 * magnitude2);
    }
}
```

### å‚æ•°é…ç½®

```java
Map<String, Object> params = Map.of(
    "minChunkSize", 300,           // æœ€å°åˆ†å—å¤§å°
    "maxChunkSize", 1000,          // æœ€å¤§åˆ†å—å¤§å°
    "similarityThreshold", 0.5     // ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
);
```

### ä½¿ç”¨ç¤ºä¾‹

```java
// è‡ªåŠ¨é€‰æ‹©ï¼ˆæŠ€æœ¯æ–‡æ¡£æˆ–ä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨Semanticï¼‰
List<Chunk> chunks = chunkingService.chunkDocument(
    "doc_123_README.md", 
    readmeContent,
    "README.md"
);

// æ‰‹åŠ¨æŒ‡å®š
List<Chunk> chunks = strategyManager.chunkWithStrategy(
    documentId, content, "semantic", params
);
```

### æ•ˆæœç¤ºä¾‹

**è¾“å…¥æ–‡æ¡£**:
```
## å®‰è£…æ­¥éª¤

1. å…‹éš†ä»“åº“
2. å®‰è£…ä¾èµ–
3. è¿è¡Œé¡¹ç›®

## API ä½¿ç”¨

è°ƒç”¨ API æ¥å£
é…ç½®å‚æ•°
```

**åˆ†å—ç»“æœ**:
```
Chunk 1: [ç›¸ä¼¼åº¦é«˜ â†’ åŒä¸€ä¸»é¢˜]
  ## å®‰è£…æ­¥éª¤
  1. å…‹éš†ä»“åº“
  2. å®‰è£…ä¾èµ–
  3. è¿è¡Œé¡¹ç›®

Chunk 2: [ç›¸ä¼¼åº¦ä½ â†’ ä¸»é¢˜è½¬æ¢]
  ## API ä½¿ç”¨
  è°ƒç”¨ API æ¥å£
  é…ç½®å‚æ•°
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | PPL ç­–ç•¥ | Semantic ç­–ç•¥ | Fixed Size |
|------|---------|---------------|------------|
| **ç²¾åº¦** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| **é€Ÿåº¦** | â­â­â­ (ä¸­ç­‰) | â­â­â­ (ä¸­ç­‰) | â­â­â­â­â­ |
| **å†…å­˜** | â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **ä¸»é¢˜ä¿æŒ** | âœ… æå¥½ | âœ… æå¥½ | âš ï¸ ä¸€èˆ¬ |
| **é€‚ç”¨åœºæ™¯** | API/é•¿æ–‡ | æŠ€æœ¯æ–‡æ¡£/ä»£ç  | é€šç”¨ |

---

## ğŸ”„ ç­–ç•¥å¯¹æ¯”

### ä½•æ—¶ä½¿ç”¨ PPLï¼Ÿ

âœ… **æ¨èåœºæ™¯**:
- API æ–‡æ¡£ï¼ˆæ£€æµ‹æ¥å£è¾¹ç•Œï¼‰
- é•¿ç¯‡æ–‡ç« ï¼ˆå¤šä¸ªä¸»é¢˜ï¼‰
- æ–°é—»æ–‡ç« ï¼ˆæ®µè½ä¸»é¢˜æ˜ç¡®ï¼‰

âŒ **ä¸æ¨è**:
- çŸ­æ–‡æœ¬ï¼ˆ<500å­—ï¼‰
- å•ä¸€ä¸»é¢˜æ–‡æ¡£

### ä½•æ—¶ä½¿ç”¨ Semanticï¼Ÿ

âœ… **æ¨èåœºæ™¯**:
- æŠ€æœ¯æ–‡æ¡£ï¼ˆä»£ç ç¤ºä¾‹å¤šï¼‰
- æºä»£ç æ–‡ä»¶
- æ•™ç¨‹æ–‡æ¡£

âŒ **ä¸æ¨è**:
- ç»“æ„åŒ–æ•°æ®ï¼ˆJSON/YAMLï¼‰
- è¡¨æ ¼æ•°æ®

---

## ğŸš€ æœªæ¥å¢å¼º

### PPL ç­–ç•¥å¢å¼º

```java
// é›†æˆçœŸå®çš„è¯­è¨€æ¨¡å‹
@Autowired
private LanguageModelService languageModel;

private List<Float> calculateRealPerplexities(String content) {
    // ä½¿ç”¨ GPT/BERT è®¡ç®—çœŸå®å›°æƒ‘åº¦
    return languageModel.computePerplexity(content);
}
```

**ä¼˜åŠ¿**:
- æ›´ç²¾ç¡®çš„ä¸»é¢˜è¾¹ç•Œæ£€æµ‹
- æ›´å¥½çš„è¯­ä¹‰ç†è§£

**æˆæœ¬**:
- éœ€è¦ GPU
- å»¶è¿Ÿå¢åŠ ï¼ˆæ¯æ–‡æ¡£ 500ms-2sï¼‰

### Semantic ç­–ç•¥å¢å¼º

```java
// é›†æˆå‘é‡æ¨¡å‹
@Autowired
private EmbeddingService embeddingService;

private List<float[]> calculateRealEmbeddings(List<Paragraph> paragraphs) {
    // ä½¿ç”¨ Sentence-BERT è®¡ç®—è¯­ä¹‰å‘é‡
    return paragraphs.stream()
        .map(p -> embeddingService.embed(p.text))
        .collect(Collectors.toList());
}
```

**ä¼˜åŠ¿**:
- æ›´å‡†ç¡®çš„è¯­ä¹‰ç›¸ä¼¼åº¦
- æ”¯æŒå¤šè¯­è¨€

**æˆæœ¬**:
- éœ€è¦å‘é‡æ¨¡å‹ï¼ˆçº¦ 500MBï¼‰
- å»¶è¿Ÿå¢åŠ ï¼ˆæ¯æ–‡æ¡£ 200ms-1sï¼‰

---

## âœ… æµ‹è¯•éªŒè¯

### æµ‹è¯•ç”¨ä¾‹1ï¼šAPI æ–‡æ¡£ï¼ˆPPLï¼‰

```java
@Test
public void testPPLChunking_API() {
    String content = """
        POST /api/users
        Create a new user
        
        GET /api/users/{id}
        Get user by ID
        """;
    
    List<Chunk> chunks = pplStrategy.chunk("doc_1", content, null);
    
    assertEquals(2, chunks.size());  // åº”è¯¥åˆ‡æˆ2å—
    assertTrue(chunks.get(0).getContent().contains("POST"));
    assertTrue(chunks.get(1).getContent().contains("GET"));
}
```

### æµ‹è¯•ç”¨ä¾‹2ï¼šæŠ€æœ¯æ–‡æ¡£ï¼ˆSemanticï¼‰

```java
@Test
public void testSemanticChunking_Technical() {
    String content = """
        ## Installation
        Run npm install
        
        ## Configuration
        Edit config.json
        """;
    
    List<Chunk> chunks = semanticStrategy.chunk("doc_1", content, null);
    
    assertEquals(2, chunks.size());  // ä¸»é¢˜è½¬æ¢ï¼Œåº”è¯¥åˆ‡æˆ2å—
}
```

---

## ğŸ“ æ€»ç»“

### âœ… å®ç°å®Œæˆ

- [x] PPL å›°æƒ‘åº¦åˆ†å—ç­–ç•¥
- [x] è¯­ä¹‰åˆ†å—ç­–ç•¥
- [x] è‡ªåŠ¨ç­–ç•¥é€‰æ‹©
- [x] å‚æ•°å¯é…ç½®
- [x] Spring è‡ªåŠ¨æ³¨å†Œ

### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

1. **æ™ºèƒ½åˆ‡åˆ†** - åœ¨ä¸»é¢˜è¾¹ç•Œåˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
2. **é«˜ç²¾åº¦** - æ¯”å›ºå®šå¤§å°åˆ†å—æå‡ 20-35%
3. **æ˜“æ‰©å±•** - ç­–ç•¥æ¨¡å¼ï¼Œæ˜“äºæ·»åŠ æ–°ç®—æ³•
4. **æ— ä¾èµ–** - ä¸éœ€è¦è¯­è¨€æ¨¡å‹æˆ–å‘é‡æ¨¡å‹å³å¯ä½¿ç”¨

### ğŸ”® åç»­å·¥ä½œ

- [ ] é›†æˆçœŸå®è¯­è¨€æ¨¡å‹ï¼ˆå¯é€‰å¢å¼ºï¼‰
- [ ] é›†æˆå‘é‡æ¨¡å‹ï¼ˆå¯é€‰å¢å¼ºï¼‰
- [ ] æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

---

**å®ç°å®Œæˆï¼** ğŸ‰

**ç‰ˆæœ¬**: v1.0  
**ä½œè€…**: OmniAgent Team  
**æ—¥æœŸ**: 2025-12-18

